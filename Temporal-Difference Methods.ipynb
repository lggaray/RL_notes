{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson covers material in Chapter 6 (especially 6.1-6.6) of the [textbook](http://go.udacity.com/rl-textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental difference between MC and TD\n",
    "* Monte Carlo learning __needed breaks__(between episodes), it needed the episode to end so that the return could be calculated, and then used as an estimate for the action values.\n",
    "\n",
    "\n",
    "## Examples for real world problem \n",
    "* If an agent is playing chess, instead of waiting until the end of an episode to see if it's won the game or not, it will at every move be able to estimate the probability that it's winning the game, or a self-driving car at every turn will be able to estimate if it's likely to crash, and if necessary, amend a strategy to avoid disaster.\n",
    "\n",
    "* To emphasize, the Monte Carlo approach would have **this car crash every time it wants to learn anything**, and its too expensive and also quite dangerous.\n",
    "\n",
    "* __TD__ learning will amend its prediction at every step.\n",
    "* And we can solve both continuous and episodic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Control: Sarsa\n",
    "\n",
    "`TD = Monte Carlo + Bellman Equation`\n",
    "\n",
    "Monte Carlo (MC) control methods require us to complete an entire episode of interaction before updating the Q-table. Temporal Difference (TD) methods will instead update Q-table after every step.\n",
    "\n",
    "* In TD will update the Q-table at every step in the episode.\n",
    "* We basically use the Bellman equation when doing sarsa update.\n",
    "* So while updating the current value in the Q-table, *we add reward of that state which get taking the particular action and the value of next state-action pair according to policy.\n",
    "\n",
    "\n",
    "\n",
    "### Math behind TD Equation:\n",
    "Lets start with Monte Carlo control equations\n",
    "> $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha (G_t - Q(S_t,A_t)$\n",
    "<br>$G_t$: alternative estimate (`uses for loop till terminal state to add up all rewards`)\n",
    "<br>$Q(S_t,A_t)$:current estimate\n",
    "\n",
    "**For temporal-Difference control**\n",
    "> $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha (R_{t+1}+\\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)))$\n",
    "<br>$(R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})$: alternative estimate\n",
    "<br>$Q(S_t,A_t)$:current estimate\n",
    "\n",
    "\n",
    "\n",
    "With the exception of this new update step, its identical to what we did in the Monte Carlo case.\n",
    "\n",
    "In particular, we'll use the Epsilon greedy policy to select actions at every time step.\n",
    "\n",
    "The only real difference is that we update the Q table at every time step instead of waiting until the end of the episode, and as long as we specify appropriate values for Epsilon the algorithm is guaranteed to converge to optimal policy.\n",
    "\n",
    "Name of this algorithm is **SARSA 0**\n",
    "\n",
    "The name of this algorithm is Sarsa zero also known as Sarsa for short the name comes form the fact that each action value update uses a __(state, action, reward, next state, next action)__ tuple of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa(0)\n",
    "\n",
    "In this algorithm, the number of episodes the agent collects is equal to *num_episodes*. For every time step $t \\ge 0$, the agent:\n",
    "* takes the action $A_t$ (from the current state $S_t$) that is $\\epsilon$-greedy with respect to the Q-table,\n",
    "* receives the reward $R_{t+1}$ and next state $S_{t+1}$,\n",
    "* chooses the next action $A_{t+1}$ (from the next state $S_{t+1}$) that is $\\epsilon$- greedy with respect to the Q-table,\n",
    "* uses the information in the tuples $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ to update the entry $Q(S_t,A_t)$ in the Q-table corresponding to the current state $S_t$ and the action $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Control: Q-Learning (or Sarsamax)\n",
    "\n",
    "Check out this [research paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7501&rep=rep1&type=pdf) to read proof that Q-learning (or Sarsamax) converges.\n",
    "\n",
    "We'll still begin with the same initial values for the action values and the policy.The agent receives the initial state, the first action is still chosen form the initial policy. But then after receiving the reward and next state, we're going to do something else.\n",
    "\n",
    "Namely, __we'll update the policy before choosing the next action__.\n",
    "In particular, consider using the action from Greedy policy, instead of the Epsilon Greedy policy.In fact this is what Sarsamax or Q-learning does.\n",
    "\n",
    "And so what happens is after we update the action value for time step zero using the greedy action, we then select select A1 using the Epsilon greedy policy corresponding to the action values we just updated. And this continues when we received a reward and next state. Then, we do the same thing we did before where we update the value corresponding to S1 and A1 using the greedy action, then we select A2 corresponding Epsilon greedy policy.\n",
    "\n",
    "\n",
    "## Difference Sarsa(0) and Sarsamax(Q-learning).\n",
    "* We update the state-action value (Q-table) with greedy action in the next state $max_{a \\in A}Q(S_{t+1},a)$.\n",
    "* But we use $\\epsilon-$greedy policy to take the next step with Q-table.\n",
    "* In sarsa(0) we used $\\epsilon-$greedy policy action's to update the Q-table unlike sarsamax.\n",
    "\n",
    "\n",
    "> $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha (R_{t+1}+\\gamma max_{a \\in A}Q(S_{t+1},a)  - Q(S_t,A_t))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 + (-1 +(9) -6)*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Control : Expected Sarsa\n",
    "\n",
    "Check out this (optional) [research paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.216.4144&rep=rep1&type=pdf) to learn more about Expected Sarsa.\n",
    "\n",
    "Expected Sarsa does something a bit different, It uses the expected value of the next state action pair, where the expectation takes into account the probability that the agent selects each possible action from the next state.\n",
    "\n",
    "## Equation for Expected Sarsa\n",
    "> $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha (R_{t+1} + \\gamma \\sum_{a \\in A} \\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t,A_t))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## expected sarsa calculations\n",
    "0.4/4\n",
    "1 - 0.4 + 0.4/4\n",
    "6 + (-1 +(0.1*(8+7+8)+ 0.7*(9)) -6 )*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Control: Theory and Practice\n",
    "\n",
    "\n",
    "## Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "***\n",
    "\n",
    "The **Greedy in the limit with infinite  Exploration (GLIE)** conditions were introduced in the previous lesson, when we learned about MC control. There are many ways to satisfy the GLIE conditions, all of which involve gradually decaying the value of $\\epsilon$ when constructing $\\epsilon-$greedy policies.\n",
    "\n",
    "In particular, let $\\epsilon_{i}$ corresponds to the $i-$th time step. Then, to satisfy the GLIE conditions, we need only set $\\epsilon_i$ such that:\n",
    "* $\\epsilon > 0$ for all time steps $i$, and\n",
    "* $\\epsilon_i$ decays to zero in the limit as the time step $i$ approaches infinity (that is $lim_{i\\to \\inf} \\epsilon_i = 0 $\n",
    "\n",
    "\n",
    "## In Theory\n",
    "***\n",
    "\n",
    "All of the TD control algorithm we have examined (Sarsa, Sarsamax, Expected Sarsa) are **guaranteed to converge** to the optimal action-value function $q_*$, as long as the step size parameter $\\alpha$ is sufficiently small, and GLIE conditions are met.\n",
    "\n",
    "Once we had a good estimate for $q_*$, a corresponding optimal policy $\\pi_*$ can then be quickly obtain by setting $\\pi_*(s) = argmax_{a \\in A(s)}q_*(s,a)$ for all $s \\in S$\n",
    "\n",
    "\n",
    "## In Practice\n",
    "***\n",
    "In practice, it is common to completely ignore the GLIE conditions and still recover an optimal policy.\n",
    "\n",
    "\n",
    "## Optimism\n",
    "***\n",
    "\n",
    "We have learned  that for any TD control method, we must begin by initializing the values in the Q-table. It has been shown that [initializing the estimate to large values](http://papers.nips.cc/paper/1944-convergence-of-optimistic-and-incremental-q-learning.pdf) can improve performance. For instance, if all of the possible rewards that can be received by the agent are negative, then initializing every estimate in the Q-table to zeros is a good technique. In this case, we refer to the initializing Q-table as **optimistic**, since the action-value estimates guaranteed to be larger than the true action values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym: CliffWalkingEnv\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "This is a simple implementation of the Gridworld Cliff\n",
    "    reinforcement learning task.\n",
    "    Adapted from Example 6.6 from Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto:\n",
    "    http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\n",
    "\n",
    "    With inspiration from:\n",
    "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
    "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
    "        [3, 0] as the start at bottom-left\n",
    "        [3, 11] as the goal at bottom-right\n",
    "        [3, 1..10] as the cliff at bottom-center\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward \n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(3,15)\n",
    "\n",
    "np.arange(3,15)[::-1] ## reverses the list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa(0)\n",
    "```python\n",
    "def generate_episodes(env,Q,i):\n",
    "    state = env.reset()\n",
    "    action = get_action(env,Q,state,i)\n",
    "    episode = []\n",
    "    for ii in range(700):\n",
    "    ##while True:\n",
    "        next_state,reward,done,prob=env.step(action)\n",
    "        next_action = get_action(env,Q,next_state,i)\n",
    "        episode.append((state,action,next_state,next_action,reward))\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_action(env,Q,state,i):\n",
    "    nA = env.action_space.n\n",
    "    epsilon = 1.0/i\n",
    "    probs = epsilon*(np.ones(nA))/nA\n",
    "    probs[np.argmax(Q[state])]+= 1-epsilon\n",
    "    return np.random.choice(np.arange(nA),p=probs)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    # loop over episodes\n",
    "    tmp = []\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        \n",
    "        if i_episode == 1:\n",
    "            scores =[]\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            if scores:\n",
    "                tmp.append(np.mean(scores))\n",
    "                #print(tmp)\n",
    "            scores = []\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            #print(episode)\n",
    "            sys.stdout.flush()   \n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        episode = generate_episodes(env,Q,i_episode)\n",
    "        counter = 0\n",
    "        for j,(state,action,next_state,next_action,reward) in enumerate(episode):\n",
    "            counter += reward\n",
    "            Q[state][action] +=  alpha * (reward + (gamma**j) * Q[next_state][next_action] - Q[state][action])\n",
    "        scores.append(counter)    \n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(tmp)-3,endpoint=False),np.asarray(tmp[3:]))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % 100)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % 100), np.max(tmp))\n",
    "        \n",
    "    return Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(5)\n",
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env,5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_sarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "plot_values(V_sarsa)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsamax (Q-Learning)\n",
    "```python\n",
    "def generate_episodes(env,Q,i):\n",
    "    state = env.reset()\n",
    "    action = get_action(env,Q,state,i)\n",
    "    episode = []\n",
    "    for ii in range(700):\n",
    "    ##while True:\n",
    "        next_state,reward,done,prob=env.step(action)\n",
    "        next_action = get_action(env,Q,next_state,i)\n",
    "        episode.append((state,action,next_state,next_action,reward))\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_action(env,Q,state,i):\n",
    "    nA = env.action_space.n\n",
    "    epsilon = 1.0/i\n",
    "    probs = epsilon*(np.ones(nA))/nA\n",
    "    probs[np.argmax(Q[state])]+= 1-epsilon\n",
    "    return np.random.choice(np.arange(nA),p=probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    # loop over episodes\n",
    "    tmp = []\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        \n",
    "        if i_episode == 1:\n",
    "            scores =[]\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            if scores:\n",
    "                tmp.append(np.mean(scores))\n",
    "                #print(tmp)\n",
    "            scores = []\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            #print(episode)\n",
    "            sys.stdout.flush()   \n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        episode = generate_episodes(env,Q,i_episode)\n",
    "        counter = 0\n",
    "        for j,(state,action,next_state,next_action,reward) in enumerate(episode):\n",
    "            counter += reward\n",
    "            Q[state][action] +=  alpha * (reward + (gamma**j) * Q[next_state][np.argmax(Q[next_state])] - Q[state][action])\n",
    "        scores.append(counter)    \n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(tmp),endpoint=False),np.asarray(tmp))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % 100)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % 100), np.max(tmp))\n",
    "        \n",
    "    return Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsamax = q_learning(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsamax = np.array([np.argmax(Q_sarsamax[key]) if key in Q_sarsamax else -1 for key in np.arange(48)]).reshape((4,12))\n",
    "check_test.run_check('td_control_check', policy_sarsamax)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsamax)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_sarsamax[key]) if key in Q_sarsamax else 0 for key in np.arange(48)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa\n",
    "\n",
    "```python\n",
    "def generate_episode(env,Q,i):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    action,_ = get_action(env,Q,state,i)\n",
    "    for ii in range(1500):\n",
    "    #while True:\n",
    "        next_state,reward,done,prob = env.step(action)\n",
    "        next_action,expected_prob =get_action(env,Q,next_state,i)\n",
    "        episode.append((state,action,next_state,next_action,reward,expected_prob))\n",
    "                                            \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "    \n",
    "def get_action(env,Q,state,i):\n",
    "    nA = env.action_space.n\n",
    "    epsilon =1.0/np.sqrt(i)\n",
    "    probs = epsilon*(np.ones(nA))/nA\n",
    "    probs[np.argmax(Q[state])]+= 1-epsilon\n",
    "    return np.random.choice(np.arange(nA),p=probs),probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # loop over episodes\n",
    "    tmp = []\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        if i_episode == 1:\n",
    "            scores= []\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            if scores:\n",
    "                tmp.append(np.mean(scores))\n",
    "            scores = []\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        episode = generate_episode(env,Q,i_episode)\n",
    "        counter = 0\n",
    "        for j,(state,action,next_state,next_action,reward,expected_prob) in enumerate(episode):\n",
    "            counter+= reward\n",
    "            Q[state][action] += alpha*(reward+((gamma**j)*np.dot(Q[next_state],expected_prob))-Q[state][action])\n",
    "        scores.append(counter)\n",
    "        \n",
    "    #plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(tmp),endpoint=False),np.asarray(tmp))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)'%100)\n",
    "    plt.show()\n",
    "    #print best 100-episode performance\n",
    "    print(\"Best Average Reward over {} Episodes: {}\".format(100,np.max(tmp)))\n",
    "        \n",
    "    return Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_expsarsa = expected_sarsa(env, 10000, 0.01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_expsarsa = np.array([np.argmax(Q_expsarsa[key]) if key in Q_expsarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_expsarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_expsarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_expsarsa[key]) if key in Q_expsarsa else 0 for key in np.arange(48)])\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images\\a12.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
