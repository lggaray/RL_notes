{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THERE IS ALWAYS A TRADE OFF BETWEEN **EXPLORATION** AND **EXPLOITATION**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# The RL Framework: The Problem\n",
    "- __Learn how to mathematically formulate tasks as *Markov Decision Processes*__.\n",
    "\n",
    "\n",
    "* Agent learns from trial and error and maximizing the cumulative rewards.\n",
    "\n",
    "### Assumption\n",
    "* Time evolve in __discrete__ time steps.\n",
    "* In general, *we don't need to assume that the environment shows the agent everything he needs to make well-informed decisions.*\n",
    "* We will assume that __agent fully observes what ever state the environment is in__. \n",
    "\n",
    "\n",
    "**There is a difference between observation and environment state!**\n",
    "\n",
    "* The agent interacts with environment and gets a *state*(observation) and then agent produces and action and gets a reward form the environment according to the action. And the previous state and action decides the next state and the action can have long term effect on the environment.\n",
    "\n",
    "* MDPs are a classical formalization of sequential decision making, where actions influence not just the immediate rewards, but also the **subsequent** situations, or states, and through those future rewards.\n",
    "\n",
    "* Thus MDPs involve delayed reward and the need to **tradeoff** immediate and delayed reward.\n",
    "\n",
    "* The next state depends on the previous state and action taken by the agent.\n",
    "\n",
    "\n",
    "### Goal of the Agent:\n",
    "* Maximize the **expected** cumulative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic vs. Continuous Tasks\n",
    "\n",
    "* Episodic Task- Interaction ends at some time step $T$.\n",
    "    * One interaction from start to finish is know as **Episode**.\n",
    "    * Reborns in same environment with added experience.\n",
    "\n",
    "* Continuing Task: Interaction continues with no ending.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reward Hypothesis\n",
    "\n",
    "* **Reinforcement** - \"It refers to stimulus that's delivered immediately after behaviour more likely to occur in the future.\"\n",
    "\n",
    "**All goals can be framed as the maximization of *expected* cumulative reward.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Reward\n",
    "\n",
    "**Could agent just maximize the reward at each time step?**\n",
    "- No!\n",
    "\n",
    "* Action have short and long term consequences and the agent needs to gain some understanding of the complex effects its actions have on the environment.\n",
    "\n",
    "* So when a agent chooses a action, **How exactly does it keep all time steps in mind?**\n",
    "\n",
    "* It's important to note that the rewards for all previous time steps have already been decided as they're in the past.\n",
    "\n",
    "* Only the future rewards are inside the agent's control.\n",
    "\n",
    "* We refer to the sum of rewards from the next time step onward as the return and denote it with a capital G, and at an arbitrary time step, the agent will always choose an action towards the goal of maximizing the return.\n",
    "\n",
    "**Agent seeks to maximize the *expected* return**\n",
    "* Because agent can't predict with full certainty what will be next reward is likely be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Reward\n",
    "\n",
    "**Should present reward carry same weight as future reward?**\n",
    "\n",
    "* The main idea behind the discounted rate reward is to give present reward more importance than the future reward.\n",
    "\n",
    "\n",
    "* Could there be an agent which cares more about the future reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount rate determines the **present** value of future rewards: a reward received *k* time steps in the future is worth only $\\gamma^{k-1}$  times it would worth if it were received immediately.\n",
    "* If $\\gamma=0$ then agent is \"myopic\" and only considered about immediate reward.\n",
    "* As $\\gamma$ approaches 1, the return objective takes future rewards into account more strongly; agent becomes more farsighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "* But what's important to emphasize here is how little information the environment uses to make decisions.\n",
    "* It doesn't care what situation was present to the agent 10 or 100  or even two steps prior.\n",
    "* And it doesn't look at the actions that agent took prior to the last one.\n",
    "* And how well agent is doing or how much reward it's collected has **no effect** on how environment chooses to respond to the agent.\n",
    "\n",
    "* Of course, it's possible to design environments that have much more complex procedures for interacting with the agent, but this how its done in reinforcement learning.\n",
    "\n",
    "\n",
    "# One-Step Dynamics\n",
    "* This prove convenient to represent the environment's dynamics using mathematical notation.\n",
    "\n",
    "* When environment responds to the agent at time step $t+1$, it consider only the state and action of previous time step (S_t,A_t).\n",
    "* It doesn't care about the previous steps state and action pairs.\n",
    "* Environment has no effect by how well the agent is doing and what all rewards its collecting.\n",
    "\n",
    ">$p(s^{'},r|s,a)=P(S_{t+1} =s^{'}, R_{t+1} = r|S_t=s,A_t=a )$\n",
    "\n",
    "**WHEN THE ENVIRONMENT RESPONDS TO THE AGENT AT TIME STEP $T+1$, IT CONSIDER ONLY THE STATE AND ACTION AT THE PREVIOUS TIME STEP $(S_t,A_t)$**\n",
    "\n",
    "**And one-step dynamics returns the probability for the possible next states**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite MDPs\n",
    "* They have finite a state space and action space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "### The Setting, Revisted\n",
    "* The reinforcement learning (RL) framework is charaterized by an **Agent** learning to interact with its **environment**.\n",
    "* At each time step, the agent receives the environment 's state(environment presents a situation to the agent), and the agent must choose an approperiate **action** in response. One time step later, the agent receives a **Reward** (the environment indicates whether the agent has responded appropriatley to state) and a new **State**.\n",
    "* All agent have a goal to maximize expected **Cumulative reward**, or the expected sum of rewards attained over all time steps.\n",
    "\n",
    "\n",
    "### Episodic vs. Continuing Tasks\n",
    "* A task is an instance of the reinforcement learning (RL) problem.\n",
    "* **Continuing tasks** are tasks that continue forever, without end.\n",
    "* **Episodic tasks** are tasks with well-defined starting and ending point.\n",
    "    * In this case, we refer to a complete sequence of interaction, from start to finish, as an **episode**.\n",
    "    * Episodic tasks come to an end whenever the agent reaches a **terminal state**.\n",
    "    \n",
    "    \n",
    "    \n",
    "### The Reward Hypothesis\n",
    "* **Reward Hypothesis**: All goals can e framed as the maximization of (expected) cumulative reward.\n",
    "\n",
    "### Cumulative Reward\n",
    "* The **return at time step** t is $G_t := R_{t+1}+R_{t+2}+R_{t+3}+ ...$\n",
    "* The agent selects actions with the goal to maximize expected (discounted) return. (Note: discounting is covered in the next concept)\n",
    "\n",
    "\n",
    "### Discounted Return\n",
    "* The **discounted return at time step** t is \n",
    "    * $G_t := R_{t+1}+\\gamma R_{t+2}+ \\gamma^2 R_{t+3}+ ...$\n",
    "* The discount rate $\\gamma$ something that you set, to refine the goal that you have the agent.\n",
    "    * It must be $0 <= \\gamma <= 1$\n",
    "    * If $\\gamma = 0 $, the agent only cares about the most immediate reward.\n",
    "    * if $\\gamma = 1$, the return is not discounted.\n",
    "    * For larger values $\\gamma$, the agent cares more about the distant future. Smaller value of $\\gamma$ results in more extreme discounting, where - in the most extreme case-agent only cares about the immediate reward.\n",
    "    \n",
    "    \n",
    "### MDPs and One-Step Dynamics\n",
    "* The state space $S$ is the set of all (nonterminal) states.\n",
    "* In episodic tasks, we use $S^+$ to refer to the set of all states, including terminal states.\n",
    "* The **Action space** $A$ is the set of all possible actions.(Alternatively, $A(s)$ refers to the set of possible actions available in state $s \\in S $)\n",
    "\n",
    "* The **one step dynamics** of the environment determines how the environment decides the **state and reward** at every time step. The dynamics can be defined by specifying\n",
    "    * $p(s^{'},r|s,a)=P(S_{t+1} =s^{'}, R_{t+1} = r|S_t=s,A_t=a )$\n",
    "\n",
    "* A (finite) Markov Decision Process (MDP) is defined by:\n",
    "    * a (finite) set of states $S$(or $S^+$, in case of episodic task)\n",
    "    * a finite set of actions $A$\n",
    "    * a set of reward $R$\n",
    "    * the one-step dynamics of the environment\n",
    "    * the discount rate $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
