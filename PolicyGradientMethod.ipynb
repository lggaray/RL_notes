{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Policy Gradient Methods?\n",
    "---\n",
    "Policy gradient methods are a __subclass__ of **policy-based** methods.\n",
    "   - It estimate the weights of an optimal policy through **gradient ascent**.(Unlike hill climbing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example to better understand **Policy Gradient Ascent** and example is teaching an agent to cross a busy road.\n",
    "* Possible actions - `UP,DOWN,LEFT,RIGHT`\n",
    "* So we can represent the policy in terms of neural network and input is state space and ourput is the probability for each possible action.\n",
    "* If we're training our nn with raw-pixels then CNN is the best bet.\n",
    "* Our goal is to find the weights of the neural network that yield the optimal policy.\n",
    "\n",
    "### Working -- outline\n",
    "* So, for instance, say the agent plays the game for a single round or episode and ends up making it to the other side safely and within the time limit.\n",
    "* But then, when it plays the game for another episode, it chooses an uwise series of actions that leads to it losing the round.\n",
    "### Reward system\n",
    "* We'll give it a rewards of +1 if it crosses the road otherwise -1. And it's only delivered at the end of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture \n",
    "---\n",
    "Before digging into the details of policy gradient methods, we'll discuss how they work at high level.\n",
    "\n",
    "```\n",
    "LOOP:\n",
    " Collect an episode.\n",
    " Change the weights of the policy network:\n",
    "     if WON, increase the probability of each (state,action) combination.\n",
    "     if LOST,decrease the probability of each (state,action) combination\n",
    "\n",
    "\n",
    "```\n",
    "### Summary\n",
    "1. For each episode, if the agent won the game, we'll amend the policy network weights to make each (state,action) pair that appeared in the episode to make them more **likely** to **Repeat** in future episode.\n",
    "2. For each episode, if the agent lost the game, we'll change the policy network weights to make it **less likely** to **repeat** the corresponding (state,action) pairs in future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections to Supervised Learning\n",
    "---\n",
    "Policy gradient methods are very similar to supervised learning.\n",
    "\n",
    "#### Important Difference between Policy Gradient and Supervised Learning\n",
    "* One important difference is that when we do image classification(supervised learning).Typically we work with dataset that __doesn't change over time.__(means for a image when we are training the __label will be same__ every time when ever we're passing it through neural network ulike **Policy Gradient** (Reinforcement Learning)\n",
    "* However, in the reinforcement learning setting. The dataset varies by episode.\n",
    "* So we use the policy to collect an episode, that give us a Dataset or a bunch of matched state action pairs and then we use that data set once to do a batch of updates.\n",
    "* After these updates are done we discard the datasets and then collect new set data(another episode).\n",
    "* So dataset is changing continuously, it's highly likely that we will experience a situation where the dataset has multiple conflicting opinions about what the best output should be for an input, or in other words, what the best action is to take from a game state.\n",
    "\n",
    "* __It like a same image appears in supervised learning with different labels.__\n",
    "\n",
    "* So this does make our current situation more complex.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "---\n",
    "We're now ready to get started with rigorously defining how policy gradient mehtods will work.\n",
    "\n",
    "\n",
    "### Trajectory\n",
    "* The first thing we need to define is a __Trajectory__.\n",
    "    * __Trajectory__ = state-action sequence\n",
    "    * $s_0,a_0,r_1,s_1,a_1,r_2,s_2,a_2,r_3,s_3,...$\n",
    "* But actually, a Trajectory is a little bit more **flexible** because there are no __restriction on its length__.\n",
    "* So, it can correspond to a full episode or just a small part of an episode.\n",
    "* We denote **trajactory** length with $H$, where $H$ stands for **Horizon** and denote trajactory with $\\tau$.\n",
    "* Then, the sum reward from that Trajectory is written as $R(\\tau)$.\n",
    "* Our goal in this lesson is the same as in the previous lesson.\n",
    "* We want to find the weights $\\theta$ of neural network that maximize expected return.\n",
    "* One way of accomplishing this is by setting the weights of the neural network so that on average, the agent experience trajectories that yield high return.\n",
    "* We denote the expected return by $U$ and it is a function of $\\theta$\n",
    "* We want to find the value of $\\theta$ that maximizes $U$.\n",
    "### Expected Return\n",
    ">$U(\\theta) = \\sum_{\\tau}P(\\tau;\\theta)R(\\tau)$\n",
    "    * $R(\\tau)$: Return corresponding to an arbitrary Trajectory tab.\n",
    "    * Now to take return and calculate expected return we need to calculate Probability\n",
    "    * $P(\\tau;\\theta)$: probability of trajectory $\\tau$(it depends on policy, hence $\\theta$)\n",
    "    \n",
    "## Important Note \n",
    "---\n",
    ">$U(\\theta) = \\sum_{\\tau}P(\\tau;\\theta)R(\\tau)$\n",
    "\n",
    "To see how it corresponds to the **expected return**, note that we've expressed the **return** $R(\\tau)$ as a function of the trajectory $\\tau$. Then, we calculate the weighted average (*where the weights are given by $P(\\tau;\\theta)$*)of all possible values that the return $R(\\tau)$ can take.\n",
    "\n",
    "### Why Trajectories?\n",
    "---\n",
    "We may be wondering: why are we using *trajectories* instead of episodes?The answer is that maximizing expected return over trajectories (instead of episodes) lets us search for optimal policies for both episodic and continuing task!\n",
    "\n",
    "That said, for many episodic tasks, it often makes sense to just use the full episode. In particular, for the case of the video game example described in the lessons, reward is only derivative at the end of the episode. In this case, in order to estimate the expected return, trajectory should correspond to full episode; otherwise, we don't have enough reward information to meaningfully estimate the expected return.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE \n",
    "\n",
    "We've learned that our goal is to find the values of the weights $\\theta$ in the neural network that maximize the expected return $U$.\n",
    ">$U(\\theta)= \\sum_{\\tau}P(\\tau;\\theta)R(\\tau)$\n",
    "\n",
    "<br>wher $\\tau$ is an arbitrary trajectory. One way to determine the value of $\\theta$ that maximizes this function is through **gradient ascent**. This algorithm is closely related to **gradient descent**, where the difference are that:\n",
    "   - gradient descent is designed to find the **minimum** of a function, whereas gradient ascent will find the **maximum**, and\n",
    "   - gradient descent steps in the direction of the **negative gradient**, whereas gradient ascent steps in the direction of the **gradient**.\n",
    "##### Our update step for Gradient ascent\n",
    ">$\\theta \\gets \\theta + \\alpha \\delta_{\\theta} U(\\theta)$\n",
    "* $\\alpha\\ $: is the step size that is generally allowed to decay over time. \n",
    "* Once we know how to calculate or estimate this gradient, we can repeatedly apply this update step, in the hopes that $\\theta$ converges to the value that maximizes $U(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we've learned, we can express the expected return as a probability weighted sum, where we take into account the probability of each possible trajectory and, the return permits trajectory.\n",
    "* GOAL: find $\\theta$ maximizes the expected return.\n",
    "* One way to do that is by Gradient Ascent, where we iteratively take small steps in the direction of the gradient.\n",
    "    * __Gradient Descent__ -- minimizes the function:\n",
    "        * $\\theta \\gets \\theta - \\alpha \\delta_{\\theta} U(\\theta)$\n",
    "    * __Gradient Ascent__ -- maximizes the function:\n",
    "        * $\\theta \\gets \\theta + \\alpha \\delta_{\\theta} U(\\theta)$\n",
    "* To apply above mentioned methods we need to calculate the gradient.\n",
    "* __Now, we won't be able to calculate the exact value of the GRADIENT since that is computationally too expensive.__\n",
    "    * It's computationally expensive __because__ in order to calculate the gradient exactly, we'll have to consider every possible trajectory.\n",
    "* To calculate the gradient $\\delta_{\\theta}U(\\theta)$, we have to consider every possible trajectory.\n",
    "* To **estimate** the gradient $\\delta_{\\theta}U(\\theta)$, we have to consider a **few trajectories**.\n",
    "\n",
    "\n",
    "* Specifically, we'll use the policy to collect end trajectories.\n",
    "* Use the policy $\\pi_{\\theta}$ to collect trajectories $\\tau^{(1)}, \\tau^{(2)},.....,\\tau^{(m)}$.\n",
    "* Remember, any trajectory is just the sequence of states and action and we'll use this notationtot refer to ith tajectory.\n",
    "    * $\\tau^{(i)} = (s_{0}^{(i)},a_{0}^{(i)},s_{1}^{(i)},a_{1}^{(i)},...., a_{H}^{(i)},s_{H+1}^{(i)})$\n",
    "    \n",
    "* Then we will use this trajectories to **estimate** the gradient $\\delta_{\\theta}U(\\theta)$.\n",
    "## BIG SHOT EQUATION FOR CALCULATING GRADIENT!!\n",
    "$\\delta_{\\theta} = g^{'} := \\frac{1}{m} \\sum^{m}_{i=1}\\sum^{H}_{t=0} \\delta_{\\theta} \\log \\pi_{\\theta}(a_t^{(i)}|s_t^{(i)})R(\\tau^{(i)})$\n",
    "\n",
    "* One important thing to note here, is an estimate for the gradient which we refer to as g hat(estimat if gradient) is equal to **consolidation** of information from the **M** trajectories.\n",
    "\n",
    "\n",
    "## Recap of big picture\n",
    "<br>LOOP:\n",
    "   1. Use the policy $\\pi_{\\theta}$ to collect trajectories $\\tau^{(1)}, \\tau^{(2)},.....,\\tau^{(m)}$.\n",
    "       * $\\tau^{(i)} = (s_{0}^{(i)},a_{0}^{(i)},s_{1}^{(i)},a_{1}^{(i)},...., a_{H}^{(i)},s_{H+1}^{(i)})$\n",
    "   2. Use the trajectories to estimate the gradient $\\delta_{\\theta}U(\\theta)$:\n",
    "       * $\\delta_{\\theta} = g^{'} := \\frac{1}{m} \\sum^{m}_{i=1}\\sum^{H}_{t=0} \\delta_{\\theta} \\log \\pi_{\\theta}(a_t^{(i)}|s_t^{(i)})R(\\tau^{(i)})$\n",
    "   3. Update the weights of the policy:\n",
    "       * $\\theta \\gets \\theta + \\alpha g^{'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving deep into Gradient Equation\n",
    "## Equation:\n",
    "$\\delta_{\\theta} = g^{'} := \\frac{1}{m} \\sum^{m}_{i=1}\\sum^{H}_{t=0} \\delta_{\\theta} \\log \\pi_{\\theta}(a_t^{(i)}|s_t^{(i)})R(\\tau^{(i)})$\n",
    "### Initial Idea\n",
    "* Change the Policy weight to accomodate the following condition:\n",
    "    * If __WON__, increase the probability of each (state,action) combination (sequence) in the trajectory which ended in winning.\n",
    "    * If __LOST__, decrease the probability of each (state,action) combination in the trajetory which combination (sequence) ended in losing.\n",
    "    \n",
    "## Initial Assumption (simplifying the problem )\n",
    "1. Only one __trajectory__ for calculating the gradient (for weight updation)\n",
    "    * In terms of our gradient equation m=1.\n",
    "2. And trajectory $\\tau$ corresponds to **full episode**\n",
    "\n",
    "### Now the equation for simplified problem:\n",
    "$\\delta_{\\theta} = g^{'} = \\sum_{t=0}^{H} \\delta_{\\theta}\\log \\pi_{\\theta}(a_t|s_t)R(\\tau)$\n",
    "* We currently assuming that we have calculate the full episode which we refered as to $\\tau$.\n",
    "* $R(\\tau)$ is just __cumulative reward__ from that trajectory.\n",
    "* Now in our current example, the reward singal and the sample game we're working with, gives the agent a reward of __positive one__ if we won the game and a reward of __minus one__ we lost the game. \n",
    "* And $\\tau$ is just a sequence of states and actions.\n",
    "* And $\\pi_{\\theta}(a_t|s_t)$ term looks at the probability of that the agent selects action $a_t$ in states $s_t$\n",
    "* And $\\pi_{\\theta}$ refers to policy which agent follows and policy depends on the **Weights** $\\theta$.\n",
    "* Then this full calculation here, takes the gradient of the **log** of that probability.\n",
    "* This will tell us how we should change the weights of the policy theta, if we want to increase the log probability of selecting action $a_t$ in state $s_t$.\n",
    "\n",
    "\n",
    "* Specifically, if we nudge the weight in the direction of gradient then we increases the log probability of selecting that action in that state space **and** if we step in the opposite direction will decrease the log proability of selecting that action in that space.\n",
    "\n",
    "\n",
    "## Basic Working\n",
    "* The basic idea here is that if **WON**, then cumulative reward +1 so gradient is positive it will nudge weight in direction of gradient and hence will increase the probability of taking that action in that state and if __LOST__, cumulative reward -1 so gradient is negative it will nudge the weight in the direction opposite to the gradient and hence minimize it, will decrease the probability of taking that action in that state.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.logger.set_level(40) ##supress warnings\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How categorical works\n",
    "\n",
    "```python\n",
    "    >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
    "    >>> m.sample()  # equal probability of 0, 1, 2, 3\n",
    "     3\n",
    "    [torch.LongTensor of size 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env= gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('observation space: ', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(s_size,h_size)\n",
    "        self.fc2 = nn.Linear(h_size,a_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs =self.forward(state).cpu()\n",
    "#         print(probs)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(),m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "24\n",
      "12\n",
      "10\n",
      "8\n",
      "11\n",
      "12\n",
      "18\n",
      "12\n",
      "9\n",
      "36\n",
      "11\n",
      "31\n",
      "11\n",
      "13\n",
      "9\n",
      "27\n",
      "8\n",
      "17\n",
      "37\n",
      "9\n",
      "31\n",
      "9\n",
      "13\n",
      "19\n",
      "21\n",
      "14\n",
      "18\n",
      "13\n",
      "9\n",
      "12\n",
      "16\n",
      "23\n",
      "37\n",
      "18\n",
      "10\n",
      "14\n",
      "20\n",
      "9\n",
      "14\n",
      "23\n",
      "22\n",
      "8\n",
      "39\n",
      "16\n",
      "8\n",
      "15\n",
      "9\n",
      "10\n",
      "12\n",
      "15\n",
      "69\n",
      "20\n",
      "18\n",
      "10\n",
      "10\n",
      "24\n",
      "52\n",
      "40\n",
      "11\n",
      "38\n",
      "14\n",
      "32\n",
      "14\n",
      "21\n",
      "16\n",
      "22\n",
      "26\n",
      "20\n",
      "14\n",
      "18\n",
      "12\n",
      "21\n",
      "37\n",
      "10\n",
      "22\n",
      "33\n",
      "32\n",
      "10\n",
      "45\n",
      "23\n",
      "17\n",
      "14\n",
      "30\n",
      "13\n",
      "14\n",
      "27\n",
      "80\n",
      "13\n",
      "19\n",
      "28\n",
      "34\n",
      "19\n",
      "20\n",
      "19\n",
      "14\n",
      "30\n",
      "25\n",
      "50\n",
      "34\n",
      "Episode 100\tAverage Score: 21.68\n",
      "17\n",
      "76\n",
      "27\n",
      "31\n",
      "29\n",
      "64\n",
      "41\n",
      "56\n",
      "56\n",
      "28\n",
      "46\n",
      "24\n",
      "31\n",
      "16\n",
      "21\n",
      "65\n",
      "33\n",
      "33\n",
      "13\n",
      "48\n",
      "19\n",
      "48\n",
      "54\n",
      "44\n",
      "14\n",
      "19\n",
      "24\n",
      "30\n",
      "29\n",
      "40\n",
      "44\n",
      "23\n",
      "20\n",
      "153\n",
      "57\n",
      "23\n",
      "42\n",
      "69\n",
      "60\n",
      "57\n",
      "12\n",
      "72\n",
      "20\n",
      "48\n",
      "29\n",
      "66\n",
      "34\n",
      "38\n",
      "48\n",
      "74\n",
      "35\n",
      "18\n",
      "16\n",
      "37\n",
      "18\n",
      "27\n",
      "16\n",
      "44\n",
      "12\n",
      "20\n",
      "18\n",
      "26\n",
      "11\n",
      "23\n",
      "28\n",
      "12\n",
      "13\n",
      "10\n",
      "12\n",
      "19\n",
      "16\n",
      "28\n",
      "23\n",
      "15\n",
      "13\n",
      "13\n",
      "41\n",
      "10\n",
      "14\n",
      "35\n",
      "39\n",
      "54\n",
      "13\n",
      "10\n",
      "8\n",
      "23\n",
      "21\n",
      "17\n",
      "35\n",
      "36\n",
      "27\n",
      "30\n",
      "48\n",
      "37\n",
      "22\n",
      "44\n",
      "39\n",
      "27\n",
      "33\n",
      "33\n",
      "Episode 200\tAverage Score: 33.84\n",
      "28\n",
      "19\n",
      "17\n",
      "38\n",
      "24\n",
      "62\n",
      "26\n",
      "21\n",
      "48\n",
      "39\n",
      "78\n",
      "17\n",
      "36\n",
      "18\n",
      "19\n",
      "19\n",
      "16\n",
      "30\n",
      "65\n",
      "27\n",
      "159\n",
      "52\n",
      "31\n",
      "29\n",
      "38\n",
      "18\n",
      "67\n",
      "22\n",
      "16\n",
      "26\n",
      "29\n",
      "25\n",
      "91\n",
      "12\n",
      "54\n",
      "125\n",
      "37\n",
      "47\n",
      "32\n",
      "43\n",
      "21\n",
      "28\n",
      "92\n",
      "79\n",
      "52\n",
      "56\n",
      "43\n",
      "22\n",
      "27\n",
      "20\n",
      "19\n",
      "29\n",
      "34\n",
      "88\n",
      "78\n",
      "71\n",
      "76\n",
      "37\n",
      "95\n",
      "118\n",
      "55\n",
      "41\n",
      "60\n",
      "100\n",
      "67\n",
      "135\n",
      "89\n",
      "90\n",
      "40\n",
      "91\n",
      "70\n",
      "60\n",
      "51\n",
      "38\n",
      "82\n",
      "75\n",
      "122\n",
      "143\n",
      "120\n",
      "45\n",
      "49\n",
      "23\n",
      "167\n",
      "66\n",
      "90\n",
      "58\n",
      "199\n",
      "169\n",
      "199\n",
      "69\n",
      "87\n",
      "19\n",
      "155\n",
      "70\n",
      "98\n",
      "38\n",
      "133\n",
      "123\n",
      "41\n",
      "125\n",
      "Episode 300\tAverage Score: 63.37\n",
      "166\n",
      "113\n",
      "42\n",
      "109\n",
      "114\n",
      "99\n",
      "115\n",
      "114\n",
      "77\n",
      "130\n",
      "111\n",
      "16\n",
      "41\n",
      "36\n",
      "39\n",
      "94\n",
      "10\n",
      "48\n",
      "28\n",
      "78\n",
      "68\n",
      "85\n",
      "59\n",
      "85\n",
      "65\n",
      "111\n",
      "88\n",
      "42\n",
      "57\n",
      "68\n",
      "81\n",
      "68\n",
      "97\n",
      "82\n",
      "98\n",
      "97\n",
      "90\n",
      "83\n",
      "79\n",
      "71\n",
      "84\n",
      "82\n",
      "31\n",
      "96\n",
      "65\n",
      "110\n",
      "121\n",
      "66\n",
      "71\n",
      "46\n",
      "59\n",
      "99\n",
      "65\n",
      "42\n",
      "66\n",
      "68\n",
      "71\n",
      "67\n",
      "57\n",
      "26\n",
      "26\n",
      "59\n",
      "93\n",
      "66\n",
      "64\n",
      "50\n",
      "61\n",
      "69\n",
      "103\n",
      "87\n",
      "140\n",
      "62\n",
      "113\n",
      "103\n",
      "199\n",
      "60\n",
      "78\n",
      "199\n",
      "126\n",
      "117\n",
      "86\n",
      "98\n",
      "112\n",
      "199\n",
      "140\n",
      "144\n",
      "120\n",
      "127\n",
      "199\n",
      "103\n",
      "160\n",
      "127\n",
      "98\n",
      "164\n",
      "158\n",
      "115\n",
      "105\n",
      "152\n",
      "199\n",
      "107\n",
      "Episode 400\tAverage Score: 92.34\n",
      "64\n",
      "96\n",
      "128\n",
      "115\n",
      "154\n",
      "155\n",
      "111\n",
      "199\n",
      "143\n",
      "93\n",
      "199\n",
      "122\n",
      "199\n",
      "121\n",
      "125\n",
      "199\n",
      "189\n",
      "199\n",
      "153\n",
      "199\n",
      "182\n",
      "144\n",
      "181\n",
      "126\n",
      "126\n",
      "128\n",
      "56\n",
      "165\n",
      "150\n",
      "136\n",
      "134\n",
      "71\n",
      "125\n",
      "125\n",
      "85\n",
      "107\n",
      "95\n",
      "67\n",
      "80\n",
      "56\n",
      "64\n",
      "21\n",
      "71\n",
      "84\n",
      "34\n",
      "95\n",
      "66\n",
      "71\n",
      "52\n",
      "54\n",
      "52\n",
      "34\n",
      "75\n",
      "83\n",
      "99\n",
      "95\n",
      "81\n",
      "130\n",
      "67\n",
      "81\n",
      "84\n",
      "79\n",
      "122\n",
      "95\n",
      "108\n",
      "54\n",
      "117\n",
      "125\n",
      "113\n",
      "164\n",
      "133\n",
      "139\n",
      "177\n",
      "132\n",
      "199\n",
      "140\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "166\n",
      "199\n",
      "199\n",
      "199\n",
      "172\n",
      "132\n",
      "158\n",
      "199\n",
      "98\n",
      "199\n",
      "113\n",
      "146\n",
      "142\n",
      "120\n",
      "177\n",
      "109\n",
      "185\n",
      "131\n",
      "Episode 500\tAverage Score: 128.30\n",
      "151\n",
      "160\n",
      "155\n",
      "140\n",
      "106\n",
      "145\n",
      "195\n",
      "178\n",
      "199\n",
      "121\n",
      "152\n",
      "199\n",
      "133\n",
      "199\n",
      "199\n",
      "199\n",
      "111\n",
      "199\n",
      "144\n",
      "199\n",
      "199\n",
      "117\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "198\n",
      "199\n",
      "199\n",
      "176\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "193\n",
      "199\n",
      "195\n",
      "199\n",
      "199\n",
      "199\n",
      "168\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "Episode 600\tAverage Score: 191.57\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "199\n",
      "Environment solved in 508 episodes!\tAverage Score: 195.19\n"
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(),lr=1e-2)\n",
    "\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action,log_prob = policy.act(state)\n",
    "#             print(action)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward* (gamma**t))\n",
    "            if done:\n",
    "#                 print(t)\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        R = sum(rewards)\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob[0]*R)\n",
    "        policy_loss = sum(policy_loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy.state_dict(),'policy_G_cartpole.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "policy.load_state_dict(torch.load('policy_G_cartpole.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Big Idea\n",
    "---\n",
    "Is to change this problem into a classification problem and then solve it with using supervised learning technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(s_size,h_size)\n",
    "        self.fc2 = nn.Linear(h_size,8)\n",
    "        self.fc3 = nn.Linear(8,a_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs =self.forward(state).cpu()\n",
    "        #m = Categorical(probs)\n",
    "        #action = m.sample()\n",
    "        action = np.argmax(probs.detach().numpy())\n",
    "        return action,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 24.56\n",
      "Episode 200\tAverage Score: 21.08\n",
      "Episode 300\tAverage Score: 19.99\n",
      "Episode 400\tAverage Score: 21.02\n",
      "Episode 500\tAverage Score: 20.93\n",
      "Episode 600\tAverage Score: 20.79\n",
      "Episode 700\tAverage Score: 20.79\n",
      "Episode 800\tAverage Score: 18.25\n",
      "Episode 900\tAverage Score: 17.99\n",
      "Episode 1000\tAverage Score: 16.95\n",
      "Episode 1100\tAverage Score: 17.13\n",
      "Episode 1200\tAverage Score: 15.65\n",
      "Episode 1300\tAverage Score: 16.14\n",
      "Episode 1400\tAverage Score: 16.69\n",
      "Episode 1500\tAverage Score: 16.69\n",
      "Episode 1600\tAverage Score: 15.36\n",
      "Episode 1700\tAverage Score: 15.90\n",
      "Episode 1800\tAverage Score: 16.10\n",
      "Episode 1900\tAverage Score: 16.93\n",
      "Episode 2000\tAverage Score: 17.66\n",
      "Episode 2100\tAverage Score: 16.01\n",
      "Episode 2200\tAverage Score: 15.62\n",
      "Episode 2300\tAverage Score: 16.28\n",
      "Episode 2400\tAverage Score: 17.34\n",
      "Episode 2500\tAverage Score: 17.25\n",
      "Episode 2600\tAverage Score: 16.90\n",
      "Episode 2700\tAverage Score: 17.02\n",
      "Episode 2800\tAverage Score: 17.50\n",
      "Episode 2900\tAverage Score: 17.98\n",
      "Episode 3000\tAverage Score: 20.45\n",
      "Episode 3100\tAverage Score: 18.41\n",
      "Episode 3200\tAverage Score: 19.10\n",
      "Episode 3300\tAverage Score: 15.10\n",
      "Episode 3400\tAverage Score: 16.55\n",
      "Episode 3500\tAverage Score: 16.56\n",
      "Episode 3600\tAverage Score: 15.41\n",
      "Episode 3700\tAverage Score: 17.98\n",
      "Episode 3800\tAverage Score: 15.72\n",
      "Episode 3900\tAverage Score: 16.39\n",
      "Episode 4000\tAverage Score: 17.71\n",
      "Episode 4100\tAverage Score: 17.80\n",
      "Episode 4200\tAverage Score: 16.67\n",
      "Episode 4300\tAverage Score: 17.25\n",
      "Episode 4400\tAverage Score: 18.35\n",
      "Episode 4500\tAverage Score: 15.08\n",
      "Episode 4600\tAverage Score: 14.41\n",
      "Episode 4700\tAverage Score: 16.00\n",
      "Episode 4800\tAverage Score: 18.31\n",
      "Episode 4900\tAverage Score: 15.94\n",
      "Episode 5000\tAverage Score: 16.04\n",
      "Episode 5100\tAverage Score: 15.71\n",
      "Episode 5200\tAverage Score: 17.87\n",
      "Episode 5300\tAverage Score: 18.83\n",
      "Episode 5400\tAverage Score: 16.16\n",
      "Episode 5500\tAverage Score: 17.92\n",
      "Episode 5600\tAverage Score: 17.41\n",
      "Episode 5700\tAverage Score: 18.52\n",
      "Episode 5800\tAverage Score: 17.50\n",
      "Episode 5900\tAverage Score: 15.51\n",
      "Episode 6000\tAverage Score: 16.39\n",
      "Episode 6100\tAverage Score: 16.58\n",
      "Episode 6200\tAverage Score: 16.40\n",
      "Episode 6300\tAverage Score: 17.97\n",
      "Episode 6400\tAverage Score: 17.57\n",
      "Episode 6500\tAverage Score: 15.24\n",
      "Episode 6600\tAverage Score: 18.53\n",
      "Episode 6700\tAverage Score: 16.59\n",
      "Episode 6800\tAverage Score: 17.71\n",
      "Episode 6900\tAverage Score: 17.60\n",
      "Episode 7000\tAverage Score: 16.37\n",
      "Episode 7100\tAverage Score: 17.89\n",
      "Episode 7200\tAverage Score: 16.45\n",
      "Episode 7300\tAverage Score: 17.90\n",
      "Episode 7400\tAverage Score: 16.86\n",
      "Episode 7500\tAverage Score: 16.57\n",
      "Episode 7600\tAverage Score: 18.69\n",
      "Episode 7700\tAverage Score: 19.02\n",
      "Episode 7800\tAverage Score: 14.88\n",
      "Episode 7900\tAverage Score: 18.51\n",
      "Episode 8000\tAverage Score: 16.07\n",
      "Episode 8100\tAverage Score: 15.87\n",
      "Episode 8200\tAverage Score: 16.50\n",
      "Episode 8300\tAverage Score: 17.34\n",
      "Episode 8400\tAverage Score: 16.26\n",
      "Episode 8500\tAverage Score: 16.89\n",
      "Episode 8600\tAverage Score: 17.63\n",
      "Episode 8700\tAverage Score: 15.34\n",
      "Episode 8800\tAverage Score: 15.46\n",
      "Episode 8900\tAverage Score: 17.34\n",
      "Episode 9000\tAverage Score: 16.55\n",
      "Episode 9100\tAverage Score: 16.36\n",
      "Episode 9200\tAverage Score: 17.72\n",
      "Episode 9300\tAverage Score: 16.94\n",
      "Episode 9400\tAverage Score: 17.44\n",
      "Episode 9500\tAverage Score: 16.33\n",
      "Episode 9600\tAverage Score: 18.01\n",
      "Episode 9700\tAverage Score: 14.50\n",
      "Episode 9800\tAverage Score: 16.04\n",
      "Episode 9900\tAverage Score: 17.07\n",
      "Episode 10000\tAverage Score: 20.53\n"
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(),lr=0.002)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def reinforce(n_episodes=10000, max_t=100, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    time_trainer = 10\n",
    "    \n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        saved_action = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action,prob= policy.act(state)\n",
    "#             print(prob)\n",
    "#             print(action)\n",
    "            saved_action.append((action,prob))\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward* (gamma**t))\n",
    "            if done:\n",
    "                ts = t\n",
    "#                 print(t)\n",
    "                break\n",
    "                \n",
    "        time_space = ts\n",
    "        #print(ts)\n",
    "        #time_trainer+=5\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        #R = sum(rewards)\n",
    "        \n",
    "        policy_loss = []\n",
    "#         print(time_space)\n",
    "#         print(time_trainer,time_space)\n",
    "    \n",
    "        for action,prob in saved_action:\n",
    "            if time_trainer <= time_space:\n",
    "                with torch.no_grad():\n",
    "                    temp = torch.tensor(prob,requires_grad=False)\n",
    "                    label = temp\n",
    "                    label[0][action] = 1.0\n",
    "                loss = criterion(-prob,label)\n",
    "                \n",
    "#                 time_trainer = max(time_trainer+10,time_space)\n",
    "            else:\n",
    "#                 print(0)\n",
    "                with torch.no_grad():\n",
    "                    temp = torch.tensor(prob,requires_grad=False)\n",
    "                    label = temp\n",
    "                    label[0][action] = 0.0\n",
    "                loss = criterion(prob,label)\n",
    "#                 time_trainer = time_space\n",
    "            policy_loss.append(loss)\n",
    "        policy_loss = sum(policy_loss)\n",
    "        time_trainer = time_trainer+4\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In this lesson, we've learned all about the __REINFORCE__ algorithm, which was illustrated with a toy environment with a **discrete** action space. But it's also important to mention that REINFORCE can also be used to solve environment with continuous action spaces!\n",
    "\n",
    "For an environment wiht a continuous action space, the corresponding policy network could have an output layer that parameterizes a **continuous probability distribution**.\n",
    "\n",
    "For instance, assume the output layer returns the mean $\\mu$ and variance $\\sigma^2$ of a **normal distribution**.\n",
    "\n",
    "Then in order to select an action, the agent needs only to pass the most recent state $s_t$ as input to the network, and then use the output mean $\\mu$ and variance $\\sigma^2$ to sample from the distribution $a_t - N(\\mu,\\sigma^2)$\n",
    "\n",
    "This should work in theory, but it's unlikely to perform well in practice! To improve performance with continuous action spaces, we'll have to make some small modification to the REINFORCE algorithm, and we'll learn more about these modification in the upcoming lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## What are Policy Gradient Methods?\n",
    "***\n",
    "   - **Policy-based methods** are a class of algorithm that search directly for the optimal policy, without simultaneously maintaining value function estimation.\n",
    "   - **Policy gradient methods** are a subclass of policy based methods that estimate the weights of an optimal policy through gradient ascent.\n",
    "   - In this lesson, we represent the policy with a neural network, where our goal is to find the weights $\\theta$ of the network that maximize expected return.\n",
    "   \n",
    "   \n",
    "## The Big Picture\n",
    "***\n",
    "* The policy gradient method will iteratively amend the policy network weights to:\n",
    "    * make (state, action) pairs that resulted in positive return more likely, and\n",
    "    * make (state, action) pairs that result in negative return less likely.\n",
    "    \n",
    "## Problem Setup\n",
    "***\n",
    "* A **trajectory** $\\tau$ is state-action sequence $s_0,a_,0....s_H,a_H,s_{H+1}$.\n",
    "* In this lesson, we will use the notation $R(\\tau)$ to refer to the return corresponding to trajectory $\\tau$.\n",
    "* Our goal is to find the weight $\\theta$ of the policy network to maximize the **expected** return $U(\\theta) := \\sum_{\\tau}P(\\tau;\\theta)R(\\tau)$\n",
    "\n",
    "## REINFORCE\n",
    "***\n",
    "* The pseudocode for REINFORCE is as follows:\n",
    "    1. Use the policy $\\pi_\\theta$ to collect $m$ trajectories $\\{\\tau^{1},\\tau^{2}...,\\tau^{m}\\}$ with horizon H. We refer to the $i$-th trajectory as\n",
    "        * $\\tau^{i} = s_0^{i},a_0^{i},....s_H^{i},a_H^{i},s_{H+1}^{i}$\n",
    "     2. Use the trajectories to estimate the gradient $\n",
    "     \\delta_{\\theta}U(\\theta)=g^{'}:=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{t=0}^{H}\\delta_{\\theta}\\log\\pi_\\theta(a_t^{i}|s_t^{i})R(\\tau^{i})$\n",
    "     3. Update the weights of the policy:\n",
    "         * $\\theta \\gets \\theta + \\alpha g^{'}$\n",
    "     4. Loop over steps 1-3\n",
    "     \n",
    "## Derivation\n",
    "***\n",
    "* We derived the **likelihood ration policy gradient**:\n",
    "    * $\\delta_{\\theta}U(\\theta) = \\sum_{\\tau}P(\\tau;\\theta)\\delta_{\\theta}\\log P(\\tau;\\theta)R(\\theta)$\n",
    "* We can approximate the gradient above with a sample-weighted average:\n",
    "    * $\\delta_{\\theta}U(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\delta_{\\theta}\\log P(\\tau^{(i)};\\theta)R(\\tau^{(i)})$\n",
    "* We can calculate the following:\n",
    "    * $\\delta_{\\theta}\\log P(\\tau^{(i)};\\theta) = \\sum_{t=0}^{H}\\delta_{\\theta}\\log\\pi_\\theta (a_t^{(i)}|s_t^{(i)})$\n",
    "\n",
    "## What's Next?\n",
    "***\n",
    "* Reinforce can solve Markov Decision Processe (MDPs) with either discrete or continuous action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
