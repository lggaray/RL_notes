{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "* Actor Critic methods are at the intersection of __value based methods__(DQN) and **policy based methods**(REINFORCE)\n",
    "* If an agent uses the neural network to approximate value function then it is called value based methods.\n",
    "* If an agent uses the neural network to approximate policy of the environment then it is called policy based method.\n",
    "* The dqn agent we learned about uses neural network to approximate **optimal action value** function. \n",
    "* Value Based methods can be used to find following value function\n",
    "    * $V_{\\pi}(s)$ -- __state-value__\n",
    "    * $Q_\\pi(s,a)$ -- __action-value__\n",
    "    * $A_\\pi(s,a)$ -- __advantage function__\n",
    "    * optimal version is denoted by $V_*$ and so on.\n",
    "* __Stochastic Policy__: takes in the state and gives probability distribution of all possible action.\n",
    "* __Determinstic Policy__: takes in the state and outputs the single action for any given state.\n",
    "* __Problem__ with __policy based methods__ are **high variance**.\n",
    "### Main idea of ActorCritic Method\n",
    "* Use value based method to reduce variance of policy based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance(Important)\n",
    "* There is always a trade off between **bias and variance**.\n",
    "<img src = \"images/a16.png\">\n",
    "* __1 Quadrant__ : HIGH BIAS, HIGH VARIANCE\n",
    "* __2 Quadrant__ : LOW BIAS, HIGH VARIANCE\n",
    "* __3 Quadrant__ : LOW BIAS, LOW VARIANCE -- _desired_\n",
    "* __4 Quadrant__ : HIGH BIAS, LOW VARIANCE\n",
    "\n",
    "\n",
    "\n",
    "* An agent tries to estimate value function or policies from returns;  a return(cumulative reward over trakectories) is calculated using a single trajectories, **however value functions which is what we're trying to estimate are calculate using the EXPECTATION.\n",
    "* BIG PART OF REINFORCEMENT LEARNING RESEARCH IS TO REDUCE THE VARIANCE WHILE CALCULATING __RETURN__.\n",
    "\n",
    "* Simple Return over a trajectory $G_t = R_{t+1}+R_{t+2}+R_{t+3}+ .. + R_T$\n",
    "* __State value__: $V_\\pi(s) = E_\\pi[G_t|S_t=s]$\n",
    "* __Action value__: $Q_\\pi(s,a) = E_\\pi[G_t|S_t = s,A_t=a]$\n",
    "* __Advantage value__: $A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$\n",
    "\n",
    "\n",
    "* __Goal__ : To reduce the variance keeping the bias minimum.\n",
    "\n",
    "* __Reinfocement Learning agent tries to find policies(optimal) to maximize the total expected reward but we're limited to sampling the environment we can only estimate this expectation the question is what's the best way to estimate value function for our actor critic methods.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Ways For Estimating Expected Returns\n",
    "* Mote-Carlo estimate -- **not biased but high variance**\n",
    "* Temporal Difference -- **BIASED(because we use value of $Q_{t+1}$ to estimate $Q_t$) but low variance** -- **Agent will learn faster but will have problem while converging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines and Critics(Important)\n",
    "* **Monte-Carlo estimate** is __unbiased__ but __high variance__.\n",
    "* **TD-Estimate** is **biased** but **low variance**.\n",
    "* Using function approximator gives us an advantage, now we gain the power of generalization, that means when when we encounter a new state, whether we have visited that state or not but our deep neural network will still output the value(estimate) because it has been trained with similar kind of data.\n",
    "* Monte-Carlo methods have high variance but no bias.\n",
    "* TD-estimate has low variance and low bias.\n",
    "* Now word **Critic** implies that the bias has been introduced and Monte-Carlo method has no bias.\n",
    "* So instead of using Monte-Carlo estimate to train **baseline** if we use **TD-estimate**, then we can say we have a critic.\n",
    "* Sure we will be introducing the bias but **we will be reducing the variance of the model** and improving our convergence.Speeding up learning.\n",
    "* In Actor-Critic Method all we are trying to do is to reduce high variance, commonly associated with Policy Based Method.\n",
    "* **USING TD Critic instead of Monte-carlo critic will reduce the variance and thus convergence problem**\n",
    "\n",
    "\n",
    "### Takeaway\n",
    "* The important takeaway for you, though, is that there are inconsistencies out there. You often see methods named \"Actor-Critic\" when they are not. I just want to bring the issue to your attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based, Value-based and Actor Critic\n",
    "### Actor (Policy Based Approach)\n",
    "* We play a bunch of matches we then go home learns this way, we think about the matches and commit to ourselves to do more what I did in matches in which I won and less of what I did in matches I lost after many many times repeating this process we will have increase the probability of actions that led to a win and decrease the probability of actions that led to losses.\n",
    "* But we can see how this approach is rather inefficient as it needs lots of data to learn a useful policy see many of actions that occurs within a game that ended up in a loss could have **been really good actions** so decreasing the probability of good action state taking in a match only because we lost is not the best idea, sure if we repeat this process infinitely often we're likely to end up with a good policy __but at a cost of slow learning.__\n",
    "* Policy Based agent have **high variance**.\n",
    "### Critic (Value Based Approach)\n",
    "* We start playing a match and even before we get started we start **guessing** what the final score is going to be like, we continue to make guesses throughout the match, at first guesses will be off but as we get more and more experienced we will be able to make pretty solid guesses the better our guesses the better we'll tell good from bad situtations or good from bad actions the better we can make this distinction the better we'll perform of course given that we choose good actions though this not a perfect approach either guesses itroduce a bias because they'll sometimes be wrong particularly because of a lack of experience guesses are prone to under or over estimation, but guesses are **more consistent** through time.\n",
    "* If we think we are going to win a match five minutes into it, chances are you still think so **ten minutes** into it.\n",
    "* This what makes **TD-estimates** to have lower **variance**.\n",
    "\n",
    "***\n",
    "* __Policy methods__ -- Agents learn to **Act**.\n",
    "* __Value based methods__ -- Agent is learning to **Estimate stituations and actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Actor-Critic Agent\n",
    "* Actor Critic = Policy Methods + Value Based Method\n",
    "* An actor critic agent is an agent that uses function approximator to learn a policy and a value function so we will then use two neural networks one for the actor and one for the critic, the critic will learn to evaluate the state value function $v_\\pi$ using the TD estimate using the critic we will calculate the **advantage function** and train the actor using the value, a very basic online actor critic agent is as follows we have two network\n",
    "## Actor:\n",
    "* Takes in a state and **output** the distribution of actions $\\pi(a|s;\\theta_{\\pi})$\n",
    "## Critic:\n",
    "* Takes in a state and __outputs state value function__ $V(s;\\theta_v)$\n",
    "## Algorithm\n",
    "* Inputs the `current state` into the **Actor** and get the __action__ to take in that **State**, observe `next state` and `reward` to get our experience tuples $(s,a,r,s^{'})$.\n",
    "* And then using the **TD-estimate** which is reward $R$ plus the **Critic** estimate for s prime($s^{'}$) so $r + \\gamma V(s^{'};\\theta_{v})$(label) we **TRAIN** critic.\n",
    "* To calculate the **advantage** $A(s,a) = r + \\gamma V(s^{'};\\theta_v) - V(s;\\theta_v)$ we use **critic**\n",
    "* And finally we train the **Actor** Using **advantage** as a baseline.\n",
    "\n",
    "<img src = \"images/a17.png\">\n",
    "### Notes\n",
    "One important thing to note here is that we use $V(s;\\theta_v)$ or $A(s,a)$, but sometimes $V_\\pi(s'\\theta_v)$ or $A_\\pi(s,a)$\n",
    "<br>There are 2 things actually going on in there.\n",
    "1. A very common thing we'll see in reinfocement learning is the oversimplification of notation. However, both syles, whether we see $A(s,a)$ or $A_\\pi(s,a)$(value functions with or without a $\\pi$) it means we are evaluating a value function of policy $\\pi$. In case of $A$, the advantage function. A different case would be when we see a subscript $*$. For example, $A^*(s,a)$ means the optimal advantage function.Q-learning learns the optimal action-value function, $Q^*(s,a)$ for example.\n",
    "2. The other thing is the use of $\\theta_v$ in some value functions and not in others. This only means that such value function is using a neural network. For example, $V(s;\\theta_v)$ is using a neural network as function approximator, but $A(s,a)$ is not. We are calculating the advantage function $A(s,a)$ using the state-value function $V(s;\\theta_v)$, but $A(s,a)$ is not using the function approximator directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C: Asynchronous Advantage Actor-Critic, N-step Bootstrapping\n",
    "* We will be calculating the advantage function $A(s,a) = r + \\gamma V(s^{'};\\theta_v) - V(s;\\theta_v)$ and __critic__ will be learning to estimate (state value)$V_\\pi(s;\\theta_v)$.\n",
    "* If we are using images as inputs to our agent **A3C** can use a single CNN with actor and critic sharing weights in two seperate heads one for the actor and one for the critic, note that **A3C** is not to be used **Exclusively** for CNN and images but if were to ise it sharing weights is a more efficient more complex approach and can be harder to train.\n",
    "* It's a good idea to start with seperate set of weights(two networks) at beginning and change only to improve performance.\n",
    "##### Important\n",
    "* Instead of using **TD-estimate** in **A3C** we use **N-step Bootstrapping**\n",
    "    * **N-step Bootstrapping** is simply a abstraction and a generalization of the **TD** and **Mote Carlo estimate** \n",
    "* **TD** is one step bootstrapping our agent goes out and experiences **one time of real reward** and then bootstraps right there.(low variance, but biased)\n",
    "* **Monte Carlo** goes out all the way(to terminal state) and it does not bootstrap.(high variance, but no bias)Beacause it doesn't need to, Monte Carlo estimate is an infinite step bootstrapping but how about going more than **one** step but not all the way out.\n",
    "* Can we do two time steps of real reward and then bootstrap from the second next state can we do three, how about four or more we sure can this what is call **N-step Bootstrapping**.\n",
    "<img src = \"images/a18.png\">\n",
    "* **A3C** uses this type of return to **train** the __critic__.\n",
    "* For example in our tennis example and N-step bootstrapping means we will wait for sometime before guessing what the final score will look like waiting for experience the environment for a little longer before we make any prediction or we calculate the expected return of the original state allows us to have **less bias** in our prediction keeping __variance__ under control.\n",
    "* In practice only a few steps out say __four or five steps__ bootstrapping are often the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C : Asynchronous Advantage Actor - Critic, Parallel Training \n",
    "* Unlike DQN **A3C** does not use a replay buffer the main reason we needed Replay buffer was so that we break the **correlation between sequential states**.\n",
    "* And we replay buffer we can randomly select experience tuple from replay buffer and break the correlation beween sequential experience tuples and randomly select the experience tuples and put them in minibatch and also can use the old experience.\n",
    "<img src = \"images/a19.png\">\n",
    "\n",
    "### A3C replacement for Replay Buffer\n",
    "* __A3C__ replaces replay buffer with parallel training by creating mutliple instances of the the environment and agent and running them all at the same time our agent will receive mini batches of the decorrelated experiences just as we need samples de correlated because agents will likely be experienced in different state at any given time.\n",
    "* This type of training allows us to use __on policy__ learning in our learning algorithm which often associated with more __stable__ learning. \n",
    "<img src = \"images/a20.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C: Asynchronous Advantage Actor-Critic, Off-policy vs. On-policy\n",
    "* __On-policy__: Policy used for interacting with the environment is also the policy being __learned__(or being optimizied to get optimal policy).\n",
    "* __Off-policy__: Policy used for interacting with the environment is __different__ than the policy being learned.\n",
    "* **Sarsa** is a good example of **on-policy** and **Sarsamax**(Q-Learning) is a good example of **off-policy**\n",
    "    * A q-learning agent learns about the agent learns about the optimal policy though the policy **generates behaviour is an exploratory policy** often **epsilon-greedy policy** \n",
    "* **Q-learning** learns a **optimal deterministic policy** even if its behaviour policy is totally **stochastic**(random).\n",
    "* **Sarsa** learns the best **Exploratory** policy that is the best policy that still explores.\n",
    "### Update Equations:\n",
    "#### Sarsa\n",
    "$$Q(S,A) \\gets Q(S,A) + \\alpha [R + \\gamma Q(S^{'},A^{'}) - Q(S,A)]$$\n",
    "#### Q-learning\n",
    "$$Q(S,A) \\gets Q(S,A) + \\alpha [R + \\gamma max_a Q(S^{'},a) - Q(S,A)]$$\n",
    "\n",
    "* DQN is also a off-policy learning method our agent behaves with some exploratory policy say Epsilon greedy but it learns the optimal policy when using off-policy learning agents are able to __learn from many different sources__ including experiences generated by all versions of the agent itself thus the replay buffer.\n",
    "* However off policy learning is know to be **unstable** and often **Diverge** with deep neural networks.\n",
    "* **A3C** on the other hand is a __on-policy__ method with on policy learning we only use the data generated by the policy currently being learned about and anytime we improve the policy we __toss out__ all data and go out collect some more.\n",
    "* __On-policy__ learning is a bit inefficient in the use of experiences but it often has more stable and consistent convergence properties.\n",
    "* A simple analogy of on and off policy goes this way:\n",
    "    * __On-policy__ is learning from your own hands-on experience for example project in this nanodegree as we can imagine that is a pretty good way of learning but it is somewhat data inefficient we can only do so many projects before we run out time.\n",
    "    * __Off-Policy__ on other hand is learning from someone's experience and as such it more sample efficient because well we can learn from many different sources however this way of learning is more prone to misunderstandings. Other people might not able to explain things in a way we understand well.\n",
    "* The nanodegree analogy in the **off-policy** case is learning from watching the lesson for example we learn much faster this way but again perhaps not as good and deep we would from self-study or own hands-on experience.\n",
    "* Usually a good balance between **off-policy** and **on-policy** is desired.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If we want to learn about the agent that combines on and off policy  learning then read the paper by **Google** title [Q-prop](https://arxiv.org/abs/1611.02247) \n",
    "<br>**Q-prop sample efficient policy gradient** with an **off-policy** Critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C: Advantage Actor-Critic\n",
    "* What is asynchronous part in __A3C__ is all about.\n",
    "* __A3C__ accumulates gradient update and applies those updates **asynchronously** to a global neural network.\n",
    "* Each agent in simulation does this at its own time so the agent use a local copy of the network to collect experience calculate and accumulate gradient across multiple time steps and they apply these gradients to a gloabl network asynchronoulsy.\n",
    "* Asynchronous here mean that agent will update the network on its own there is no synchornization between the agents this also means that the weights the agent is using might be different from the weight in use by another agent at any given time.\n",
    "\n",
    "\n",
    "\n",
    "There is a **synchronous** implementation of the **A3C** known as **A2C**. It has some extra bit of code that synchronizes all agents it wait for all agents to finish a segment of iteraction with its copy of the environment and then updates the network at once before sending the updated weights back to all agent.\n",
    "<br>__A2C__ arguably simpler to implement and its gives pretty much the same result and allegedly in some cases performes even better.\n",
    "<br>__A3C__ most easily trained on a CPU while A2C is more straightforward to extend on GPU implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAE: Generalized Advantage Estimation\n",
    "* There is another way for estimating expected returns call the **lambda return** the intution goes this way\n",
    "* Say after we try and step bootsrapping we realize that numbers of n larger than one often perform better but it sill hard to tell what that number should be, should it be two or three or something else.\n",
    "* To make decision even more difficult in some problem small number of n are better while in some cases large number of n are better.\n",
    "* How do we get this right, the idea of the lambda return is to create a mixture of all n step bootstrapping estimates at once.\n",
    "* **lambda** is a hyper parameter used for __weighing__ the combination of each n-step estimate to the lambda return.\n",
    "* Say we set lambda to $0.5$ the contribution to the lambda return would be a combination of all n step returns weighted by the exponentially decaying factor across the different n-step estimates.\n",
    "* Notice how the weight depends on the value of lambda we set and it decays exponentially at the rate of that value so for calculating the lambda return for state $s$ at timestep $T$ we would use all n step returns and multiply each of the N step return by the current corresponding weight.\n",
    "* Then add all of them, this sum will be the **lambda return for STATE S at time step T**.\n",
    "* Intrestingly when lambda is set to zero the the two step, three step and all n step return other than one step will be zero.\n",
    "* So lambda returned when lambda is set to zero will be equal to the **TD-estimate** and our lambda is set to __one__ then our **lambda return** other than the infinite step return will be equal to __zero__ so it is equivalent to **Monte Carlo estimate** \n",
    "<img src = \"images/a21.png\">\n",
    "\n",
    "\n",
    "#### GAE\n",
    "* Generalized advantage estimation is a way to train the critic with this __lambda return__.\n",
    "* We can fit the advantage function just like in a **A3C** and **A2C** but using a mixture of n-step bootstrapping estimates.\n",
    "* It;s important to highlight that this type of return can be combined with virtually any policy based method and in fact in the paper that introduced the **GAE**. **TRPO** was the policy based method used.\n",
    "* By using this type of estimation this algorithm **TRPO** plus __GAE__ trains very quickly because multiple value function are spread around every time due to the lambda return style estimate \n",
    "<img src = \"images/a22.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient, Continuous Action-Space\n",
    "* DDPG is a different type of Actor-Critic Method infact it could be seen as a kind of approximate DQN instead of actual Actor-Critic.\n",
    "* The reason for this is that the critic in DDPG is used to approximate the __maximizer over action value function__$max_aQ(s^{'},a)$ of next state and not as a learned baseline.\n",
    "* Though this is a very important algorithm.\n",
    "* One of the limitation of DQN agent is that it only work for **Discrete action space**.\n",
    "    * Because its label for training is $r + \\gamma max_a Q[s^{'},a]$ and its hard to find max value in continuous action space.\n",
    "### DDPG\n",
    "* In **DDPG** we use two network one actor and other critic.\n",
    "## Actor\n",
    "* Now actor here is used to approximate **optimal policy deterministically**.\n",
    "* We want to always output best belief action for any given state this is **unlike stochastic policy** in which we want to learn the policy to output the **probability distribution** of action.\n",
    "* In **DDPG** we want to output the best belief action every time we query the action from the network.(That is a **deterministic policy**)\n",
    "* The actor is basically learning to output $argmax_{a}Q(s,a)$ which is the best action.\n",
    "## Critic\n",
    "* Learns to evaluate the optimal action value function by using actor best belief action\n",
    "* $Q(s,\\mu(s;\\theta_\\mu);\\theta_Q)$\n",
    "\n",
    "\n",
    "\n",
    "In the [DDPG paper](https://arxiv.org/abs/1509.02971), they introduced this algorithm as an \"Actor-Critic\" method. Though, some researchers think DDPG is best classified as a DQN method for continuous action spaces (along with [NAF](https://arxiv.org/abs/1603.00748)). Regardless, DDPG is a very successful method and it's good for you to gain some intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Pipeline\n",
    "* We first input current state into actor and get an action.\n",
    "* then we take that action and get next state and reward.\n",
    "* with `torch.no_grad()` we pass in the next_state throught the critic network get the $critc(nextstate,\\theta_C)$\n",
    "* And target for critic is $reward + critc(nextstate,\\theta_C)$ with mse loss.\n",
    "* And target for actor is $Critic(currenState,\\theta_C)$ with policy gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG: Deep Deterministic Policy Gradient, Soft Updates\n",
    "* Two intresting part of DDPG are:\n",
    "    * Replay Buffer\n",
    "    * Soft Updates\n",
    "* In DQn we have two copies of our network weights, the **Regular** and **Target** Network.\n",
    "* In Atari paper where DQN were introduced the target network is updated every 10,000 time steps, we simply copy the weights of regular network into target network.\n",
    "* That is the target network is fixed for 10,000 time steps and then it gets a big update.\n",
    "* In DDPG we have two copies of our network weights for each network.\n",
    "* A regular for the __actor__ and a regular for the __critic__ and a target for the **actor** and a target for the **critic**.\n",
    "* But in DDPG the target networks are updated a **soft update** strategy.\n",
    "* The soft update consists of slowly blending our **regular** network weights with our target weights.\n",
    "    * Every step, mix $0.01%$ of regular network weights with target network weights.\n",
    "* Remember the regular network weight is the most update weight while target network is the one we use for pridiction to stabilize the training. \n",
    "* In practice we get faster convergence by using this update strategy and in fact this way for updating the target network weights can be used with other algorithms that use target networks including the DQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation DDPG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    \"\"\"\n",
    "        xavier initialization.\n",
    "    \"\"\"\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1./np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, fc1_units =400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__() ## initialize the nn.Module class\n",
    "        self.seed = torch.manual_seed(random_seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps state -> actions\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \"\"\" Critic (value) model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size(int): Dimension of each state\n",
    "            action_size(int): Dimension of each action\n",
    "            seed (init): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__() ##initialize the nn.Module class\n",
    "        self.seed = torch.manual_seed(random_seed)\n",
    "        self.fcs1 = nn.Linear(state_size,fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs,action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddpg Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "#from model import Actor, Critic\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "##========== HYPERPARAMETER ============##\n",
    "BUFFER_SIZE = int(1e5)    # replay buffer\n",
    "BATCH_SIZE = 128          # minibatch size\n",
    "GAMMA = 0.99              # discounting factor\n",
    "TAU = 1e-3                # soft update of traget parameters\n",
    "LR_ACTOR = 1e-4           # learning rate for actor\n",
    "LR_CRITIC = 1e-3          # learning rate for critic\n",
    "WEIGHT_DECAY = 0          # L2 weight weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = LR_ACTOR)\n",
    "        \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = LR_CRITIC, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size,random_seed)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward \n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experience = self.memory.sample()\n",
    "            self.learn(experience, GAMMA)\n",
    "            \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Return actions for given state as per current policy.\"\"\"\n",
    "        #Save experience / reward\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experience, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples\n",
    "        \n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        \n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state,action) -> Q-value\n",
    "            \n",
    "        Params\n",
    "        ======\n",
    "            experience (Torch[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        state, action, reward, next_state, done = experience\n",
    "        \n",
    "        # ============================== Update Critic =================================#\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        \n",
    "        self.actor_target.eval() ## there is no point is saving gradient\n",
    "        self.critic_target.eval()\n",
    "        \n",
    "        actions_next = self.actor_target(next_state)\n",
    "        Q_target_next = self.critic_target(next_state,actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = reward + (gamma*Q_target_next*(1-done))\n",
    "        \n",
    "        ## Compute Critic Loss\n",
    "        Q_expected = self.critic_local(state,action)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        ## Minize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ============================== Update Actor =================================#\n",
    "        ## Compute actor loss\n",
    "        action_pred  = self.actor_local(state)\n",
    "        actor_loss = -self.critic_local(state,action_pred).mean()\n",
    "        ## The reason we can calculate loss this way and we don't have\n",
    "        ## to collect trajector ( noisy Monte carlo estimation; cum_reward/reward_future)\n",
    "        ## is action space is continuous and differentiable and we calculate\n",
    "        ## gradient w.r.t to q_value which is estimated by CRITIC.\n",
    "        # Minimize loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # ========================== Update target network =================================#\n",
    "\n",
    "        self.soft_update(self.critic_local,self.critic_target,TAU)\n",
    "        self.soft_update(self.actor_local,self.actor_target,TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param,local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "    \n",
    "    \n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu*np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta*(self.mu-x) + self.sigma*np.array([random.random() for i in range(len(x))])\n",
    "        self.state =x +dx\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Important the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(2)\n",
    "agent = Agent(state_size=3, action_size=1, random_seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000, max_t = 300, print_every = 100):\n",
    "    scores_deque = deque(maxlen = print_every)\n",
    "    scores = []\n",
    "    for i_episodes in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action =agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            scores += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episodes, np.mean(scores_deque)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.sace(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episodes%print_every ==0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episodes,np.mean(scores_deque)))\n",
    "    return scores\n",
    "\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(200):\n",
    "    action = agent.act(state, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explore\n",
    "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment. To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "* Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation. Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
    "* Write your own DDPG implementation. Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
    "* You may also like to implement prioritized experience replay, to see if it speeds learning.\n",
    "* The current implementation adds Ornsetein-Uhlenbeck noise to the action space. However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that adding noise to the parameters of the neural network policy can improve performance. Make this change to the code, to verify it for yourself!\n",
    "* Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
