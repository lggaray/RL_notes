{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "* Actor Critic methods are at the intersection of __value based methods__(DQN) and **policy based methods**(REINFORCE)\n",
    "* If an agent uses the neural network to approximate value function then it is called value based methods.\n",
    "* If an agent uses the neural network to approximate policy of the environment then it is called policy based method.\n",
    "* The dqn agent we learned about uses neural network to approximate **optimal action value** function. \n",
    "* Value Based methods can be used to find following value function\n",
    "    * $$V_{\\pi}(s)$$ -- __state-value__\n",
    "    * $$Q_\\pi(s,a)$$ -- __action-value__\n",
    "    * $$A_\\pi(s,a)$$ -- __advantage function__\n",
    "    * optimal version is denoted by $$V_*$$ and so on.\n",
    "* __Stochastic Policy__: takes in the state and gives probability distribution of all possible action.\n",
    "* __Determinstic Policy__: takes in the state and outputs the single action for any given state.\n",
    "* __Problem__ with __policy based methods__ are **high variance**.\n",
    "### Main idea of ActorCritic Method\n",
    "* Use value based method to reduce variance of policy based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance(Important)\n",
    "* There is always a trade off between **bias and variance**.\n",
    "<img src = \"images/a16.png\">\n",
    "* __1 Quadrant__ : HIGH BIAS, HIGH VARIANCE\n",
    "* __2 Quadrant__ : LOW BIAS, HIGH VARIANCE\n",
    "* __3 Quadrant__ : LOW BIAS, LOW VARIANCE -- _desired_\n",
    "* __4 Quadrant__ : HIGH BIAS, LOW VARIANCE\n",
    "\n",
    "\n",
    "\n",
    "* An agent tries to estimate value function or policies from returns;  a return(cumulative reward over trakectories) is calculated using a single trajectories, **however value functions which is what we're trying to estimate are calculate using the EXPECTATION.\n",
    "* BIG PART OF REINFORCEMENT LEARNING RESEARCH IS TO REDUCE THE VARIANCE WHILE CALCULATING __RETURN__.\n",
    "\n",
    "* Simple Return over a trajectory $G_t = R_{t+1}+R_{t+2}+R_{t+3}+ .. + R_T$\n",
    "* __State value__: $V_\\pi(s) = E_\\pi[G_t|S_t=s]$\n",
    "* __Action value__: $Q_\\pi(s,a) = E_\\pi[G_t|S_t = s,A_t=a]$\n",
    "* __Advantage value__: $A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$\n",
    "\n",
    "\n",
    "* __Goal__ : To reduce the variance keeping the bias minimum.\n",
    "\n",
    "* __Reinfocement Learning agent tries to find policies(optimal) to maximize the total expected reward but we're limited to sampling the environment we can only estimate this expectation the question is what's the best way to estimate value function for our actor critic methods.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Ways For Estimating Expected Returns\n",
    "* Mote-Carlo estimate -- **not biased but high variance**\n",
    "* Temporal Difference -- **BIASED(because we use value of $Q_{t+1}$ to estimate $Q_t$) but low variance** -- **Agent will learn faster but will have problem while converging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines and Critics(Important)\n",
    "* **Monte-Carlo estimate** is __unbiased__ but __high variance__.\n",
    "* **TD-Estimate** is **biased** but **low variance**.\n",
    "* Using function approximator gives us an advantage, now we gain the power of generalization, that means when when we encounter a new state, whether we have visited that state or not but our deep neural network will still output the value(estimate) because it has been trained with similar kind of data.\n",
    "* Monte-Carlo methods have high variance but no bias.\n",
    "* TD-estimate has low variance and low bias.\n",
    "* Now word **Critic** implies that the bias has been introduced and Monte-Carlo method has no bias.\n",
    "* So instead of using Monte-Carlo estimate to train **baseline** if we use **TD-estimate**, then we can say we have a critic.\n",
    "* Sure we will be introducing the bias but **we will be reducing the variance of the model** and improving our convergence.Speeding up learning.\n",
    "* In Actor-Critic Method all we are trying to do is to reduce high variance, commonly associated with Policy Based Method.\n",
    "* **USING TD Critic instead of Monte-carlo critic will reduce the variance and thus convergence problem**\n",
    "\n",
    "\n",
    "### Takeaway\n",
    "* The important takeaway for you, though, is that there are inconsistencies out there. You often see methods named \"Actor-Critic\" when they are not. I just want to bring the issue to your attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based, Value-based and Actor Critic\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
