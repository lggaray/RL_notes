{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (finite) Markov Decision Process (MDP) is defined by:\n",
    "* A finite set of states S\n",
    "* A finite set of action A\n",
    "* A set of rewards R\n",
    "\n",
    "**One step dynamics gives the probability of Next State given the previous state and the action taken in that state.**\n",
    "\n",
    "> $p(s^{'},r|s,a) = P(S_{t+1}=s^{'},R_{t+1} = r|S_t =  s, A_t=a)$\n",
    "\n",
    "**And next state and previous state decides the Reward given to the agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "#### Deterministic Policy\n",
    "* The simplest policy is __deterministic policy__.\n",
    "    * It is the **mapping between state and action**\n",
    "    * $\\pi:S \\to A$\n",
    "    * **Deterministic** itself means one exact/definate action.\n",
    "    \n",
    "#### Stochastic Policy\n",
    "* It is another type of policy\n",
    "    * It allows policy to choose actions **Randomly**\n",
    "    * It is a mapping $\\pi:S\\times A \\to [0,1]$\n",
    "    * We define a stochastic policy as a mapping that accepts an environment state S and action A and gives **probability that agent takes action $a$ while in state $s$**(probability is given against all **possible** state actions pairs)\n",
    "    * $\\pi(a|s) = P(A_t = a|S_t = s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/a1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-value Function\n",
    "* For each state, the **state-value function** yields the __expected return__, if the agent started in that state, and then __followed the policy__ for all time steps.\n",
    "\n",
    "#### Definition\n",
    "We call $v_\\pi$ the state value function for policy $\\pi$ is \n",
    ">$V_\\pi(s) = E_\\pi[G_t|S_t =s]$\n",
    "\n",
    "**For each state $S$ it yields the `expected return` if the agent starts is state $S$ and then uses the policy to choose its actions for all time steps**\n",
    "\n",
    "### Note\n",
    "The notation $E_\\pi [.]$ is defined as the expected value of a random variable, given that the agent follows policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equations\n",
    "* In place of finding sum of all subsequent rewards after a particular state to find out its state value function.(which is redundant), we can use Bellman Equation.\n",
    "\n",
    "* Value of any state can be represented as the sum of immediate reward plus the value of state(discounted) that follows.\n",
    "\n",
    "> $v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma(v_\\pi(S_{t+1})|S_t = s)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/a2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value\n",
    "$E = \\sum x . P(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discount) value of the next state.\n",
    "\n",
    "For general MDP, we have to instead work in terms of an *expectation*, since it's not often the case that the immediate reward and the next state can be predicted with certainty. Indeed, **we saw in an earlier lesson that the reward and next state are chosen according to the ONE STEP DYNAMICS of the MDP**.In this case, where the reward r and next state s' are drawn from a (conditional) probability distribution $p(s^{'},r|s,a)$.\n",
    "The Bellman Expectation Equation (for $v_\\pi$) expresses the value of any state s in terms of the *expected* immediate reward and the *expected* value of the next state:\n",
    "> $v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma(v_\\pi(S_{t+1})|S_t = s)]$\n",
    "\n",
    "### Calculating the Expectation\n",
    "\n",
    "In the event that the agent's policy $\\pi$ is **deterministic** the agent selects actions $\\pi(s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over **two** variables (s' and r):\n",
    "\n",
    "> $V_\\pi(s) = \\sum_{s^{'} \\in S^+, r \\in R} p(s^{'}, r|s, \\pi(s))(r + \\gamma v_\\pi(s^{'}))$\n",
    "\n",
    "Because in deterministic policy $\\pi(s)$ maps the actions a, $\\pi(s) \\to a$\n",
    "\n",
    "\n",
    "If the agent's policy $\\pi$ is **stochastic**, the agent selects action a with probability $\\pi(a|s)$ when state s, and the Bellman Expectation Equation can be rewritten as the sum over three variables (s'r and a):\n",
    "\n",
    ">$V_\\pi(s) = \\sum_{s^{'} \\in S^+, r \\in R, a \\in A(s)} \\pi(a|s) p(s^{'}, r|s, a)(r + \\gamma v_\\pi(s^{'}))$\n",
    "\n",
    "In this case, we multiply the sum of the reward and discounted value of the next state $(r + \\gamma v_\\pi(s^{'}))$ by its corresponding probability $\\pi(a|s)p(s^{'},r|s,a)$ and **Sum over all possibilities** to yield the *expected value*.\n",
    "\n",
    "> All of the Bellman equations attest to the fact that value functions satisfy recursive relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality\n",
    "### Definition\n",
    "\n",
    "**$\\pi^{'} >= \\pi$ if and only if $v_{\\pi^{'}}(s) >= v_{\\pi}$ for all $s \\in S$**\n",
    "\n",
    "By definition, we say that a policy Pi-prime is better than or equal to a policy Pi if it's state-value function is greater than or eqal to that of policy Pi for **all states**\n",
    "\n",
    "#### Note \n",
    "It is often possible to find two policies that cannot be compared.\n",
    "\n",
    "But there always be a policy which will be better than or equal to all other policies. -- **Optimal Policy**\n",
    "\n",
    "**It is guaranteed to exist BUT may not be unique.**\n",
    "\n",
    "**Optimal Policy** $v_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Value Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/a3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Value function\n",
    "* For each **state** $s$ and **action** $a$ it yields the **expected return** if the agent starts in state $s$ then chooses action $a$ and then uses *the policy* to choose its action for all time steps.\n",
    "\n",
    "> $q_\\pi (s,a) = E_\\pi[G_t|S_t = s, A_t = a ]$\n",
    "\n",
    "\n",
    "Now with the action value function for each state we will need n number of values where n equals to number of possible action in the corresponding state.\n",
    "\n",
    "The **optimal action-value** function is denoted by $q_*$\n",
    "\n",
    "\n",
    "$v_\\pi(s) = q_\\pi(s, \\pi(s))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies\n",
    "\n",
    "$Interaction \\to q_* \\to \\pi_*$ \n",
    "\n",
    "* If an agent have optimal action value function then it can easily obtain optimal state value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Policies\n",
    "***\n",
    "\n",
    "* A **deterministic policy** is a mapping $\\pi: S \\to A$. For each state $s \\in S$, it yields the action $a \\in A$ that the agent will choose while in state s.\n",
    "* A **stochastic policy** is a mapping $\\pi : S \\times A \\to [0,1]$. For each state $s \\in S$ and actions $a \\in A$, it yields the probability $\\pi(a|s)$ that the agent chooses action a while in state s. \n",
    "\n",
    "\n",
    "\n",
    "### State-Value Functions\n",
    "***\n",
    "\n",
    "* The **state-value function** for a policy $\\pi$ is denoted $v_\\pi$. For each state $s \\in S$, it yields the expected return if the agent starts in state s and then uses the policy to choose its action for all time steps. That is, $v_\\pi (s) = E_\\pi[G_t|S_t = s]$. We refer to $v_\\pi(s)$ as the **value of state $s$ under policy** $\\pi$.\n",
    "\n",
    "\n",
    "### Bellman Equations\n",
    "***\n",
    "\n",
    "* The **Bellman expectation equation for $v_\\pi$** is:\n",
    "> $v_\\pi (s) = E_\\pi[R_{t+1} +\\gamma v_\\pi(S_{t+1})|S_t = s]$\n",
    "\n",
    "### Optimality\n",
    "***\n",
    "\n",
    "* A policy $\\pi ^{'}$ is defined to be better than or equal to a policy $\\pi$ if and only if $v_{\\pi^{'}}(s) >= v_{\\pi}(s)$ for all $s \\in S$.\n",
    "* An **optimal policy** $\\pi_*$ satisfies $\\pi_* >= \\pi $ for all policies $\\pi$. An optimal policy is guaranteed to exists but _may not be unique_.\n",
    "* All optimial policies have the same state-value function $v_*$, called the **optimal state-value function**.\n",
    "\n",
    "\n",
    "## Action-value Functions\n",
    "***\n",
    "\n",
    "* The **action value function** for a policy $\\pi$ is denoted $q_\\pi$. For each state $s \\in S$ and action $a \\in A$, it yields the expected return if the agent starts in state s, takes action a, and __then follows the policy for all future time steps__. That is. $q_\\pi (s,a) = E_\\pi [G_t | S_t =s, A_t = a]$. We refer to $q_\\pi(s,a)$ as the **value of taking action** a **in state** s **under policy** $\\pi$ (or alternatively as the **Value of the state-action pair** s,a ).\n",
    "\n",
    "* All optimal policies have the same action-value function $q_*$, called the **optimal action value function**.\n",
    "\n",
    "\n",
    "## Optimal Policies\n",
    "***\n",
    "\n",
    "* Once the agent determines the optimial action-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s = argmax_{a \\in A(s)} q_*(s,a))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
