{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Framework\n",
    "* Markov Decision Process (MDP): $(S,A,P,R,\\gamma)$\n",
    "* State Transition and Reward Model: $P(S_{t+1},R_{t+1}|S_t,A_t)$\n",
    "* State Value Function: V(S)\n",
    "* Action value Function: Q(S,A)\n",
    "* Goal : Find optimal policy $\\pi_*$ that maximizes total expected reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Algorithms\n",
    "* __Model-Based Learning (Dynamic Programming)__\n",
    "    * Policy Iteration\n",
    "    * Value Iteration\n",
    "    <br>**(Requires a know transition and reward model.)**\n",
    "* __Model-Free Learning__ \n",
    "    * Monte Carlo Methods\n",
    "    * Temporal-Difference Learning\n",
    "    <br>**(They sample the environment by carrying exploration actions and use the experience gained to directly estimate value functions)**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete vs. Continuous Spaces\n",
    "* States, Action: finite!!\n",
    "* If action and state space is discrete than we can map them in Q table and then can have optimal policy.\n",
    "\n",
    "$V: S \\to R$ (STATE VALUE Function)\n",
    "<br>$Q: S \\times A \\to R$(Action value function)\n",
    "\n",
    "For example in Q-Learning when we find maxQ\\[state\\] we assume the action space to be discrete.\n",
    "\n",
    "In real world we don't have discrete world like grid space, it's continuous.\n",
    "\n",
    "### Dealing with Continuous Spaces\n",
    "1. Discretization\n",
    "2. Function Approximation\n",
    "\n",
    "\n",
    "* **Discretization**: Is basically converting a continuous space into a discrete one.\n",
    "\n",
    "* __Non-Uniform Discretization__: Depends on the environment, basic idea is to recognize area with more precision where attention is required.\n",
    "* And we don't want to have smaller grid through out the space because, it will increase the number state spaces and hence computation.\n",
    "* __Read about BINARY SPACE PARTITIONING OR QUAD TREES__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization\n",
    "***\n",
    "\n",
    "In this notebook, we will deal with continuous state and action spaces by discretizing them. This will enable us to apply reinforcement learning algorithms that ae only desingned to work with discrete spaces.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plotting options \n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size = (1400, 900))\n",
    "\n",
    "is_python = 'inline' in plt.get_backend()\n",
    "if is_python:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specify the Environment, and Explore the State and Action Spaces\n",
    "We'll use OpenAI Gym environments to test and develop our algorithms. These simulate a variety of classic as well as contemporary reinforcement learning tasks. Let's use an environment that has a continuous state space, but a discrete action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment and set random seed\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(505);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state,reward,done,_ = env.step(action)\n",
    "    if done:\n",
    "        print('Score: ', t+1)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"-low:\", env.observation_space.low)\n",
    "print(\"-high:\",env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some samples from the state space\n",
    "print(\"State space samples:\")\n",
    "print(np.array([env.observation_space.sample() for i in range(10)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the action space\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "#Generate some samples from the action space\n",
    "print(\"Action space samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discrete the State Space with a Uniform Grid\n",
    "\n",
    "We will discretize the space using a uniformly-spaced grid. Implemented the following function to create such a grid, given the lower bounds(`low`), upper bounds (`high`), and number of desired `bins` along each dimension. It should return the split points for each dimension, which will be 1 less than the number of bins.\n",
    "\n",
    "For instance, if `low = [-1.0, 5.0]`,`high = [1.0,5.0]` and `bins = (10,10)`, then your function should return following list of 2 NumPy arrays:\n",
    "\n",
    "```\n",
    "[array([-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]),\n",
    " array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0])]\n",
    "```\n",
    "\n",
    "Note that the ends of `low` and `high` are **not** inculded in these split points. It is assumed that any value below the lowest split maps to index `0` and any value above the highest split point maps to index `n-1`, where `n` is the number of bins along the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0, -0.8, -0.6, -0.4, -0.2, -0.0, 0.2, 0.4, 0.6, 0.8])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.set_printoptions(precision=0, linewidth=120)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "(np.arange(-1.0,1.0,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.8, -0.6, -0.4, -0.2, -0.0, 0.2, 0.4, 0.6, 0.8]),\n",
       " array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def create_uniform_grid(low, high, bins=(10, 10)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used to discretize a space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    bins : tuple\n",
    "        Number of bins along each corresponding dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    ls = []\n",
    "    low_x,low_y=low\n",
    "    high_x,high_y = high\n",
    "    diff_x = (high_x-low_x)/bins[0]\n",
    "    diff_y = (high_y-low_y)/bins[1]\n",
    "    ls.append(np.arange(low_x,high_x,diff_x)[1:])\n",
    "    ls.append(np.arange(low_y,high_y,diff_y)[1:])\n",
    "    return ls\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "create_uniform_grid(low, high)  # [test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples:\n",
      "array([[-1.0, -5.0],\n",
      "       [-0.8, -4.1],\n",
      "       [-0.8, -4.0],\n",
      "       [-0.5, 0.0],\n",
      "       [0.2, -1.9],\n",
      "       [0.8, 4.0],\n",
      "       [0.8, 4.1],\n",
      "       [-0.5, -0.0]])\n",
      "\n",
      "Discretized samples:\n",
      "array([[0, 0],\n",
      "       [0, 0],\n",
      "       [1, 1],\n",
      "       [2, 5],\n",
      "       [6, 3],\n",
      "       [9, 9],\n",
      "       [9, 9],\n",
      "       [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    discretized_sample : array_like\n",
    "        A sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    x = np.digitize(sample[0],grid[0])\n",
    "    y = np.digitize(sample[1],grid[1])\n",
    "    return x,y\n",
    "\n",
    "\n",
    "# Test with a simple grid and some samples\n",
    "grid = create_uniform_grid([-1.0, -5.0], [1.0, 5.0])\n",
    "samples = np.array(\n",
    "    [[-1.0 , -5.0],\n",
    "     [-0.81, -4.1],\n",
    "     [-0.8 , -4.0],\n",
    "     [-0.5 ,  0.0],\n",
    "     [ 0.2 , -1.9],\n",
    "     [ 0.8 ,  4.0],\n",
    "     [ 0.81,  4.1],\n",
    "     [ -0.5 ,  -0.0]])\n",
    "discretized_samples = np.array([discretize(sample, grid) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples:\", repr(discretized_samples), sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "It might be helpful to visualize the original and discretized samples to get a sense of how much error we are introducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.collections as mc\n",
    "\n",
    "def visualize_samples(samples, discretized_samples, grid, low=None,high=None):\n",
    "    \"\"\"Visualize original and discretized samples on a given  2-dimensional grid.\"\"\"\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    \n",
    "    # Show grid\n",
    "    ax.xaxis.set_major_locator(plt.FixedLocator(grid[0]))\n",
    "    ax.yaxis.set_major_locator(plt.FixedLocator(grid[1]))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # If bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # Otherwise use first, last grid locations as low, high (for further mapping discretized samples)\n",
    "        low = [splits[0] for splits in grid]\n",
    "        high = [splits[-1] for splits in grid]\n",
    "    \n",
    "    # Map each discretized sample (which is really an index) to the center of corresponding grid cell\n",
    "    grid_extended = np.hstack((np.array([low]).T, grid, np.array([high]).T))  # add low and high ends\n",
    "    grid_centers = (grid_extended[:, 1:] + grid_extended[:, :-1]) / 2  # compute center of each grid cell\n",
    "    locs = np.stack(grid_centers[i, discretized_samples[:, i]] for i in range(len(grid))).T  # map discretized samples\n",
    "\n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o')  # plot original samples\n",
    "    ax.plot(locs[:, 0], locs[:, 1], 's')  # plot discretized samples in mapped locations\n",
    "    ax.add_collection(mc.LineCollection(list(zip(samples, locs)), colors='orange'))  # add a line connecting each original-discretized sample\n",
    "    ax.legend(['original', 'discretized'])\n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "visualize_samples(samples, discretized_samples, grid, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid discretize the state space\n",
    "state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(10,10))\n",
    "state_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain some samples from the space, discretize them,and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])\n",
    "visualize_samples(state_samples, discretized_state_samples, state_grid,\n",
    "                  env.observation_space.low, env.observation_space.high)\n",
    "plt.xlabel('position');plt.ylabel('velocity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that if we have enough bins, the discretization doesn't introduce too much error into our representation. So we may be able to now apply reinforcement learning algorithm(like Q Learning) that operates on discrete spaces. Given it a shot to see how well it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q-Learning \n",
    "Provided below is a simple Q-Learning agent. Implement the `preprocess_state()` method to convert each continuous state sample to its corresponding discretized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearningAgent():\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it\"\"\"\n",
    "    \n",
    "    def __init__(self,env,state_grid,alpha=0.02,gamma=0.99,\n",
    "                epsilon=1.0,epsilon_decay_rate = 0.9995, min_epsilon=.01,seed=505):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"\n",
    "        ## Environment info\n",
    "        self.env = env\n",
    "        self.state_grid = state_grid\n",
    "        ## FOr n dimensional state space\n",
    "        self.state_size = tuple(len(splits)+1 for splits in self.state_grid)\n",
    "        self.action_size = self.env.action_space.n # 1-dimensional discrete action space\n",
    "        self.seed = np.random.seed(seed)\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space size:\",self.state_size)\n",
    "        \n",
    "        ## Learning parameters\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.epsilon = self.initial_epsilon=epsilon # initial exploration rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate # how quickly it will decay\n",
    "        self.min_epsilon= min_epsilon\n",
    "        \n",
    "        # Create Q-table\n",
    "        self.q_table = np.zeros(shape=(self.state_size + (self.action_size,)))\n",
    "        print(\"Q table size:\", self.q_table.shape)\n",
    "        \n",
    "    def preprocess_state(self,state):\n",
    "        \"\"\"Map a continuous state to its discretized representation.\"\"\"\n",
    "        discretize_state = tuple(discretize(state,self.state_grid))\n",
    "        return discretize_state\n",
    "    \n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        # Gradually decrease exploration rate\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon,self.min_epsilon)\n",
    "        \n",
    "        # Decide inital action\n",
    "        self.last_state = self.preprocess_state(state)\n",
    "        self.last_action = np.argmax(self.q_table[self.last_state])\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        self.epsilon=epsilon if epsilon is not None else self.initial_epsilon\n",
    "        \n",
    "    def act(self,state,reward=None,done=None,mode=\"train\"):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test')\"\"\"\n",
    "        state = self.preprocess_state(state)\n",
    "        if mode =='test':\n",
    "            # Test mode: Simply produce an action\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            # Train mode (default): Update Q table, pick next action\n",
    "            # Note: We update the Q table entry for the *last* (state,action) pair with current state,reward\n",
    "            self.q_table[self.last_state+(self.last_action,)] += self.alpha* \\\n",
    "            (reward+ self.gamma*max(self.q_table[state])-self.q_table[self.last_state+(self.last_action,)])\n",
    "            \n",
    "            \n",
    "            # Exploration vs. exploitation\n",
    "            do_exploration = np.random.uniform(0,1)< self.epsilon\n",
    "            if do_exploration:\n",
    "                # Pick a random action\n",
    "                action = np.random.randint(0,self.action_size)\n",
    "            else:\n",
    "                # Pick the best action from Q table \n",
    "                action = np.argmax(self.q_table[state])\n",
    "        \n",
    "        # Roll over current state, action for next stepa\n",
    "        self.last_state = state\n",
    "        self.last_action =action\n",
    "        return action\n",
    "    \n",
    "    \n",
    "q_agent = QLearningAgent(env, state_grid)\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a convenience function to run an agent on a given environment. When calling this function, we can pass in `mode = 'test'` to tell the agent not to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, env, num_episodes=20000, mode='train'):\n",
    "    \"\"\"Run agent in given reinforcement learning environment and return scores.\"\"\"\n",
    "    scores = []\n",
    "    max_avg_score = -np.inf\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.reset_episode(state)\n",
    "        total_reward = 0\n",
    "        done= False\n",
    "        \n",
    "        # Roll out steps until done\n",
    "        while not done:\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            action = agent.act(state, reward, done, mode)\n",
    "            \n",
    "        # Save final score\n",
    "        scores.append(total_reward)\n",
    "        \n",
    "        #Print episode stats\n",
    "        if mode == 'train':\n",
    "            if len(scores)>100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "                    \n",
    "            if i_episode % 100 ==0:\n",
    "                print(\"\\rEpisode {}/{} | Max Average Score: {}\".format(i_episode,num_episodes,max_avg_score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "    return scores\n",
    "scores = run(q_agent,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to analyze if our agent was learning the task is to plot the scores. It should generally increase as the agent goes through more episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores obtained per episode \n",
    "plt.plot(scores); plt.title(\"Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the scores are noisy, it might be difficult to tell whether our agent is actually learning. To find the underlying trend, we may want to plot a rolling mean of the scores. Let's write a convenience function to plot both row scores as well as a rolling mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, rolling_window=100):\n",
    "    \"\"\"Plot scores and optional rolling mean using specified window.\"\"\"\n",
    "    plt.plot(scores);plt.title(\"Scores\");\n",
    "    rolling_mean = pd.Series(scores).rolling(rolling_window).mean()\n",
    "    plt.plot(rolling_mean);\n",
    "    return rolling_mean\n",
    "rolling_mean = plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should observe the mean of episode scores go up over time. Next, we can freeze learning and run the agent in test mode to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in test mode and analyze scores obtained\n",
    "test_scores = run(q_agent, env, num_episodes=100, mode='test')\n",
    "print(\"[TEST] Completed {} episodes with avg. scores = {}\".format(len(test_scores),np.mean(test_scores)))\n",
    "_ = plot_scores(test_scores, rolling_window = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also interesting to look at the final Q-table that is learned  by the agent. Note that the Q-table is of size $M\\times N \\times A$, where $(M,N)$ is the size of the state space, and $A$ is the size of the action space. We are intrested in the maximum Q-value for each state, and corresponding (best) action associated with that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_table(q_table):\n",
    "    \"\"\"Visualize max Q-value for each state and corresponding action.\"\"\"\n",
    "    q_image = np.max(q_table,axis=2)  # max Q-value for each state\n",
    "    q_actions = np.argmax(q_table,axis=2) # max action for each state\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    cax = ax.imshow(q_image, cmap = 'jet');\n",
    "    cbar = fig.colorbar(cax)\n",
    "    for x in range(q_image.shape[0]):\n",
    "        for y in range(q_image.shape[1]):\n",
    "            ax.text(x,y,q_actions[x,y],color=\"white\",\n",
    "                   horizontalalignment='center', verticalalignment='center')\n",
    "    ax.grid(False)\n",
    "    ax.set_title(\"Q-table, size: {}\".format(q_table.shape))\n",
    "    ax.set_xlabel('position')\n",
    "    ax.set_ylabel('velocity')\n",
    "    \n",
    "plot_q_table(q_agent.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Watch a Smart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(1000):\n",
    "    action = q_agent_new.act(state, mode='test')\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward,done,_ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        print(\"Score: \",score)\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
