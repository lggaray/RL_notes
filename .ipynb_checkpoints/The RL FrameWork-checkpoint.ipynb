{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THERE IS ALWAYS A TRADE OFF BETWEEN **EXPLORATION** AND **EXPLOITATION**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# The RL Framework: The Problem\n",
    "- __Learn how to mathematically formulate tasks as *Markov Decision Processes*__.\n",
    "\n",
    "\n",
    "* Agent learns from trial and error and maximizing the cumulative rewards.\n",
    "\n",
    "### Assumption\n",
    "* Time evolve in __discrete__ time steps.\n",
    "* In general, *we don't need to assume that the environment shows the agent everything he needs to make well-informed decisions.*\n",
    "* We will assume that __agent fully observes what ever state the environment is in__. \n",
    "\n",
    "\n",
    "**There is a difference between observation and environment state!**\n",
    "\n",
    "* The agent interacts with environment and gets a *state*(observation) and then agent produces and action and gets a reward form the environment according to the action. And the previous state and action decides the next state and the action can have long term effect on the environment.\n",
    "\n",
    "* MDPs are a classical formalization of sequential decision making, where actions influence not just the immediate rewards, but also the **subsequent** situations, or states, and through those future rewards.\n",
    "\n",
    "* Thus MDPs involve delayed reward and the need to **tradeoff** immediate and delayed reward.\n",
    "\n",
    "* The next state depends on the previous state and action taken by the agent.\n",
    "\n",
    "\n",
    "### Goal of the Agent:\n",
    "* Maximize the **expected** cumulative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic vs. Continuous Tasks\n",
    "\n",
    "* Episodic Task- Interaction ends at some time step $T$.\n",
    "    * One interaction from start to finish is know as **Episode**.\n",
    "    * Reborns in same environment with added experience.\n",
    "\n",
    "* Continuing Task: Interaction continues with no ending.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reward Hypothesis\n",
    "\n",
    "* **Reinforcement** - \"It refers to stimulus that's delivered immediately after behaviour more likely to occur in the future.\"\n",
    "\n",
    "**All goals can be framed as the maximization of *expected* cumulative reward.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Reward\n",
    "\n",
    "**Could agent just maximize the reward at each time step?**\n",
    "- No!\n",
    "\n",
    "* Action have short and long term consequences and the agent needs to gain some understanding of the complex effects its actions have on the environment.\n",
    "\n",
    "* So when a agent chooses a action, **How exactly does it keep all time steps in mind?**\n",
    "\n",
    "* It's important to note that the rewards for all previous time steps have already been decided as they're in the past.\n",
    "\n",
    "* Only the future rewards are inside the agent's control.\n",
    "\n",
    "* We refer to the sum of rewards from the next time step onward as the return and denote it with a capital G, and at an arbitrary time step, the agent will always choose an action towards the goal of maximizing the return.\n",
    "\n",
    "**Agent seeks to maximize the *expected* return**\n",
    "* Because agent can't predict with full certainty what will be next reward is likely be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Reward\n",
    "\n",
    "**Should present reward carry same weight as future reward?**\n",
    "\n",
    "* The main idea behind the discounted rate reward is to give present reward more importance than the future reward.\n",
    "\n",
    "\n",
    "* Could there be an agent which cares more about the future reward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
