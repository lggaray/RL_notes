{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews of our Notes\n",
    "***\n",
    "\n",
    "In the lesson **The RL Framework: The Problem**, we have learned how to take a real-world problem and specify it in language of reinforcement learning. In order to rigorously define reinforcement learning task, we generally use a **Markov Decision Process (MDP)** to model the environment. The MDP specifies the rules that the environment uses to responds to the agent's actions, including how much reward to give to the agent in response to its behaviour. The agent's goal is to learn how to play by the rules of the environment, in order to maximize reward.\n",
    "\n",
    "\n",
    "\n",
    "Next, in the lesson **The RL Framework:: The Solution**, we learned how to specify a solution to the reinforcement learning problem. In particular, the optimal policy $\\pi_*$ specifies -for each environment state - how the agent should select an action towards its goal of maximizing reward. We learned that the agent could structure its search for an optimal policy by first estimating the **optimal action-value function** $q_*$; once $q_*$ is known,  $\\pi_*$ is quickly obtained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld Example\n",
    "\n",
    "\n",
    "* Even we intead to move our agent in specific directions, there is no certainity(100% certain) that it will move in that direction, that is the reason we have expected state and action value and deals with probability. For example in gridworld, the floor can be slippery!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "\n",
    "* In starting of any task the agent uses the **equiproable random policy**.\n",
    "* __EQUIPROABLE RANDOM POLICY__ : When agent with equal probaility  randomly chooses any action (at random) from all possible actions space.\n",
    "\n",
    "### Basic Idea \n",
    "* We run as many episodes as we can and collect data. The data is very important because as we know not always agent moves in the direction in which it intend to move and environment can be complex. So first step is to collect data.(**infromation**)\n",
    "\n",
    "\n",
    "* __Episode__: We start the agent at the random state and takes actions according to policy (random intially with equal probability) and then try to optimize our policy to so we can get maximize cumulative reward.\n",
    "\n",
    "\n",
    "1. Information is valuable!\n",
    "2. How can the agent use it to improve its strategy?\n",
    "3. Remember: the agent is looking for the optimial policy $\\pi_*$.\n",
    "4. It tell us - for each state - which action is best.\n",
    "\n",
    "\n",
    "### For example \n",
    "If in state 0 takes action A0 and then cumulative reward till terminal state is 7.\n",
    "And if in state 0 takes actions A1 and then cumulative reward till terminal state is 6 .\n",
    "\n",
    "*So in state 0 action A0 is better choice and we update that as the policy*... and ofcourse we do this many number of episode and mean it and validate our results and then decide on the policy update.\n",
    "\n",
    "AND WE WANT TO REPEAT THIS FOR MANY EPISODE BECAUSE AGENT HASN'T TRIED ALL THE EPISODES.\n",
    "\n",
    " # THERE IS ALWAYS A TRADE OFF BETWEEN EXPLORATION AND EXPLOITATION IF WE ARE TRYING TO DO BOTH AT SAME TIME!!\n",
    " \n",
    "And we want our agent to do as much exploration it can do in initial episodes so we have all the information and our information is not **biased** and with that information, agent can exploit that information to get __optimal policy__.\n",
    "***\n",
    "\n",
    "#### Reasons for more episodes\n",
    "1. To truly understand the environment, the agent needs more episodes!\n",
    "2. The environment's dynamics are stochastic!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction\n",
    "\n",
    "<img src = images\\a4.png>\n",
    "\n",
    "The second images give us a better policy than a random policy and  its a small step towards the optimal policy.\n",
    "\n",
    "<img src = images\\a5.png>\n",
    "\n",
    "\n",
    "## Important note\n",
    "In the above picture, we demonstrated a toy example where the agent collected two episodes, consolidated the information in a table, and then used the table to come up with a better policy. However, as discussed in the previous notes, in real-world settings (and even for the toy example depicted here!), the agent will want to collect many more episodes, so that it can better trust the information stored in the table. In this video, we use two episodes only to simplify the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction -- part 2\n",
    "\n",
    "<img src = images\\a6.png>\n",
    "\n",
    "* If the agent follows a policy for many episodes, we can use the results to directly estimate the action-value function corresponding to the same policy.\n",
    "* The Q-table is used to estimate the action value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Prediction \n",
    "\n",
    "In the previous concept, we saw that estimating the action value function with a Q table is an important intermediate step. We also refer to this as **prediction problem**.\n",
    "\n",
    "**Prediction Problem**: Given a policy, how might the agent estimate the value for the policy?\n",
    "\n",
    "We refer to Monte Carlo (MC) approaches to the predicition problem as **MC prediction method**.\n",
    "\n",
    "\n",
    "### Speical Case:\n",
    "Before we dig into the pseudocode, we note that there are two different  versions of MC prediction, depending on how we decide to treat special case where - **in a single episode - the same action is selected from the same state many times**\n",
    "\n",
    "\n",
    "* Monte Carlo Methods\n",
    "    1. First-visit MC\n",
    "    2. Every-visit MC\n",
    "    \n",
    "* **First-visit MC**: average the returns following **only first visit** to a state-action pair.\n",
    "* **Every-visit MC**: average the returns following __every visit__ to a state action pair.\n",
    "\n",
    "\n",
    "### Main Idea for First visit Mc prediction\n",
    "\n",
    "* $Q$-Q-table, with a row for each state and a column for each action. The entry corresponding to state s and action a is denoted $Q(s,a)$.\n",
    "* $N-$ table that keeps track of the number of first visits we have made to each state action pair.\n",
    "* $return_sum$ -table that keeps track of sum of the rewards obtained after visits to each state-action pair.\n",
    "\n",
    "In the algorithm, the number of episodes the agen collects is equal to $num_episodes$. After each episode, N and returns_sums are updated to store infromation contained in the episode. Then, after all of the episodes have been collected and the values in N aand returns_sum have been finalized, we quickly obtain the final estimate for Q.\n",
    "\n",
    "\n",
    "## First-visit or Ever-visit?\n",
    "\n",
    "Both the first-visit and every-visit method are guaranteed to converge to the true action-value function, as the number of visits to each state-action pair approaches infinity. (So, in other words, as long as the agent gets enough experience with each state-action pair, the value function estimate will be pretty close to the true value.) In the case of first-visit MC, convergence follows from the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), and the details are covered in section 5.1 of the [textbook](http://go.udacity.com/rl-textbook).\n",
    "\n",
    "\n",
    "If you are interested in learning more about the difference between first-visit and every-visit MC methods, you are encouraged to read Section 3 of this [paper](http://www-anw.cs.umass.edu/legacy/pubs/1995_96/singh_s_ML96.pdf).The results are summarized in section 3.6. The author show:\n",
    "\n",
    "* Ever vist MC is [biased](https://en.wikipedia.org/wiki/Bias_of_an_estimator), whereas the first-visit is unbiased (see Theorems 6 and 7).\n",
    "* Initially, every visit mc has lower mean square error, but more episodes are collected, first visit MC attain better MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLACKJACK RULES\n",
    "```python\n",
    "\"\"\"Simple blackjack environment\n",
    "\n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with an infinite deck (or with replacement).\n",
    "    The game starts with each (player and dealer) having one face up and one\n",
    "    face down card.\n",
    "\n",
    "    The player can request additional cards (hit=1) until they decide to stop\n",
    "    (stick=0) or exceed 21 (bust).\n",
    "\n",
    "    After the player sticks, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "\n",
    "    The observation of a 3-tuple of: the players current sum,\n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "\n",
    "    This environment corresponds to the version of the blackjack problem\n",
    "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto (1998).\n",
    "    http://incompleteideas.net/sutton/book/the-book.html\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Code for interacting with simulator/ environment\n",
    "\n",
    "```python\n",
    "for in range(num_episode):\n",
    "    ## starting point of new episodes\n",
    "    ## we want to randomly initialize the state at each  episode\n",
    "    state = env.reset()\n",
    "    \n",
    "    ##infinite loop till we get the terminal state:\n",
    "    while True:\n",
    "        print(state)\n",
    "        ##taking action from the policy which we want to optimize here in this example we are using equiproabable random actions\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        \n",
    "        ## the step method helps us traverse over states in the environment it has the information about the current state and the action taken by the agent it will give next proabable `state`,`reward`, and whether this state is terminal state or not and info about the state.\n",
    "        state,reward,done,info = env.step(action)\n",
    "        \n",
    "        ##checking done and terminanting the infinite loop.\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :) \\n') if reward > 0 else print(\"You list :( \\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes BlackJack\n",
    "```\n",
    "OBSERVATION SPACE (current_sum(player),dealer faceup card, usable ace (T/F))\n",
    "\n",
    "STICK : STOP [0]\n",
    "HIT : CARRY ON(draw more cards) [1] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Black Jack\n",
    "\n",
    "## Part 1: MC Prediction\n",
    "\n",
    "In this section, we will write our own implementation of Mc predicition (for estimating the action-value function).\n",
    "\n",
    "We will begin by investigating a policy where the player *almost* sticks if the sum of her cards exceeds 18. In particular, she `STICKS` with 80% probability if the sum is greater than 18; and, if the sum is 18 or below , she selects action `HIT` with 80% probability. The function samples an episode using this policy.\n",
    "\n",
    "The function accepts as the **input**:\n",
    "\n",
    "* `bj_env`: This is an instance of OpenAI Gym's environment.\n",
    "\n",
    "It returns as **output**:\n",
    "* `episode`: this is list of (State,action,reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        probs = [0.8,0.2] if state > 18 else [0.2,0.8]\n",
    "        action = np.random.choice(np.arange(2),p=probs)\n",
    "        next_state,reward,done,info = bj_env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = next_state\n",
    "        if done: break\n",
    "    return episode\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "returns_sum = defaultdict(lambda:np.zeros(2))\n",
    "print(returns_sum[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way defaultdicts are defined are as follows\n",
    "\n",
    "```\n",
    "return_sum[state][actions]\n",
    "```\n",
    "so here each state(key) has two corresponding actions so it will be initialize with `a list of size 2 with 0 as the value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "## First visit MC - action value!!\n",
    "```python\n",
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        temp_list = []\n",
    "        episode= generate_episode_from_limit_stochastic(env)\n",
    "        for i,exp_tuple in enumerate(episode):\n",
    "            state,action,_ = exp_tuple\n",
    "            N[state][action] += 1.0 \n",
    "            for j,exp_tuple2 in enumerate(episode[i:]):\n",
    "                _,_,reward = exp_tuple2\n",
    "                ## makes sure it's a single visit.\n",
    "                if (state,action) not in temp_list:\n",
    "                    returns_sum[state][action] += reward * (gamma ** j)\n",
    "            Q[state][action] = returns_sum[state][action]/ N[state][action]\n",
    "            temp_list.append((state,action))         \n",
    "        \n",
    "    return Q\n",
    "```\n",
    "\n",
    "<img src \"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
