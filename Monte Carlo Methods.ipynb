{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews of our Notes\n",
    "***\n",
    "\n",
    "In the lesson **The RL Framework: The Problem**, we have learned how to take a real-world problem and specify it in language of reinforcement learning. In order to rigorously define reinforcement learning task, we generally use a **Markov Decision Process (MDP)** to model the environment. The MDP specifies the rules that the environment uses to responds to the agent's actions, including how much reward to give to the agent in response to its behaviour. The agent's goal is to learn how to play by the rules of the environment, in order to maximize reward.\n",
    "\n",
    "\n",
    "\n",
    "Next, in the lesson **The RL Framework:: The Solution**, we learned how to specify a solution to the reinforcement learning problem. In particular, the optimal policy $\\pi_*$ specifies -for each environment state - how the agent should select an action towards its goal of maximizing reward. We learned that the agent could structure its search for an optimal policy by first estimating the **optimal action-value function** $q_*$; once $q_*$ is known,  $\\pi_*$ is quickly obtained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld Example\n",
    "\n",
    "\n",
    "* Even we intead to move our agent in specific directions, there is no certainity(100% certain) that it will move in that direction, that is the reason we have expected state and action value and deals with probability. For example in gridworld, the floor can be slippery!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "\n",
    "* In starting of any task the agent uses the **equiproable random policy**.\n",
    "* __EQUIPROABLE RANDOM POLICY__ : When agent with equal probaility  randomly chooses any action (at random) from all possible actions space.\n",
    "\n",
    "### Basic Idea \n",
    "* We run as many episodes as we can and collect data. The data is very important because as we know not always agent moves in the direction in which it intend to move and environment can be complex. So first step is to collect data.(**infromation**)\n",
    "\n",
    "\n",
    "* __Episode__: We start the agent at the random state and takes actions according to policy (random intially with equal probability) and then try to optimize our policy to so we can get maximize cumulative reward.\n",
    "\n",
    "\n",
    "1. Information is valuable!\n",
    "2. How can the agent use it to improve its strategy?\n",
    "3. Remember: the agent is looking for the optimial policy $\\pi_*$.\n",
    "4. It tell us - for each state - which action is best.\n",
    "\n",
    "\n",
    "### For example \n",
    "If in state 0 takes action A0 and then cumulative reward till terminal state is 7.\n",
    "And if in state 0 takes actions A1 and then cumulative reward till terminal state is 6 .\n",
    "\n",
    "*So in state 0 action A0 is better choice and we update that as the policy*... and ofcourse we do this many number of episode and mean it and validate our results and then decide on the policy update.\n",
    "\n",
    "AND WE WANT TO REPEAT THIS FOR MANY EPISODE BECAUSE AGENT HASN'T TRIED ALL THE EPISODES.\n",
    "\n",
    " # THERE IS ALWAYS A TRADE OFF BETWEEN EXPLORATION AND EXPLOITATION IF WE ARE TRYING TO DO BOTH AT SAME TIME!!\n",
    " \n",
    "And we want our agent to do as much exploration it can do in initial episodes so we have all the information and our information is not **biased** and with that information, agent can exploit that information to get __optimal policy__.\n",
    "***\n",
    "\n",
    "#### Reasons for more episodes\n",
    "1. To truly understand the environment, the agent needs more episodes!\n",
    "2. The environment's dynamics are stochastic!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction\n",
    "\n",
    "<img src = images\\a4.png>\n",
    "\n",
    "The second images give us a better policy than a random policy and  its a small step towards the optimal policy.\n",
    "\n",
    "<img src = images\\a5.png>\n",
    "\n",
    "\n",
    "## Important note\n",
    "In the above picture, we demonstrated a toy example where the agent collected two episodes, consolidated the information in a table, and then used the table to come up with a better policy. However, as discussed in the previous notes, in real-world settings (and even for the toy example depicted here!), the agent will want to collect many more episodes, so that it can better trust the information stored in the table. In this video, we use two episodes only to simplify the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
