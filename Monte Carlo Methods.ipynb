{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews of our Notes\n",
    "***\n",
    "\n",
    "In the lesson **The RL Framework: The Problem**, we have learned how to take a real-world problem and specify it in language of reinforcement learning. In order to rigorously define reinforcement learning task, we generally use a **Markov Decision Process (MDP)** to model the environment. The MDP specifies the rules that the environment uses to responds to the agent's actions, including how much reward to give to the agent in response to its behaviour. The agent's goal is to learn how to play by the rules of the environment, in order to maximize reward.\n",
    "\n",
    "\n",
    "\n",
    "Next, in the lesson **The RL Framework:: The Solution**, we learned how to specify a solution to the reinforcement learning problem. In particular, the optimal policy $\\pi_*$ specifies -for each environment state - how the agent should select an action towards its goal of maximizing reward. We learned that the agent could structure its search for an optimal policy by first estimating the **optimal action-value function** $q_*$; once $q_*$ is known,  $\\pi_*$ is quickly obtained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld Example\n",
    "\n",
    "\n",
    "* Even we intead to move our agent in specific directions, there is no certainity(100% certain) that it will move in that direction, that is the reason we have expected state and action value and deals with probability. For example in gridworld, the floor can be slippery!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "\n",
    "* In starting of any task the agent uses the **equiproable random policy**.\n",
    "* __EQUIPROABLE RANDOM POLICY__ : When agent with equal probaility  randomly chooses any action (at random) from all possible actions space.\n",
    "\n",
    "### Basic Idea \n",
    "* We run as many episodes as we can and collect data. The data is very important because as we know not always agent moves in the direction in which it intend to move and environment can be complex. So first step is to collect data.(**infromation**)\n",
    "\n",
    "\n",
    "* __Episode__: We start the agent at the random state and takes actions according to policy (random intially with equal probability) and then try to optimize our policy to so we can get maximize cumulative reward.\n",
    "\n",
    "\n",
    "1. Information is valuable!\n",
    "2. How can the agent use it to improve its strategy?\n",
    "3. Remember: the agent is looking for the optimial policy $\\pi_*$.\n",
    "4. It tell us - for each state - which action is best.\n",
    "\n",
    "\n",
    "### For example \n",
    "If in state 0 takes action A0 and then cumulative reward till terminal state is 7.\n",
    "And if in state 0 takes actions A1 and then cumulative reward till terminal state is 6 .\n",
    "\n",
    "*So in state 0 action A0 is better choice and we update that as the policy*... and ofcourse we do this many number of episode and mean it and validate our results and then decide on the policy update.\n",
    "\n",
    "AND WE WANT TO REPEAT THIS FOR MANY EPISODE BECAUSE AGENT HASN'T TRIED ALL THE EPISODES.\n",
    "\n",
    " # THERE IS ALWAYS A TRADE OFF BETWEEN EXPLORATION AND EXPLOITATION IF WE ARE TRYING TO DO BOTH AT SAME TIME!!\n",
    " \n",
    "And we want our agent to do as much exploration it can do in initial episodes so we have all the information and our information is not **biased** and with that information, agent can exploit that information to get __optimal policy__.\n",
    "***\n",
    "\n",
    "#### Reasons for more episodes\n",
    "1. To truly understand the environment, the agent needs more episodes!\n",
    "2. The environment's dynamics are stochastic!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction\n",
    "\n",
    "<img src = images\\a4.png>\n",
    "\n",
    "The second images give us a better policy than a random policy and  its a small step towards the optimal policy.\n",
    "\n",
    "<img src = images\\a5.png>\n",
    "\n",
    "\n",
    "## Important note\n",
    "In the above picture, we demonstrated a toy example where the agent collected two episodes, consolidated the information in a table, and then used the table to come up with a better policy. However, as discussed in the previous notes, in real-world settings (and even for the toy example depicted here!), the agent will want to collect many more episodes, so that it can better trust the information stored in the table. In this video, we use two episodes only to simplify the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction -- part 2\n",
    "\n",
    "<img src = images\\a6.png>\n",
    "\n",
    "* If the agent follows a policy for many episodes, we can use the results to directly estimate the action-value function corresponding to the same policy.\n",
    "* The Q-table is used to estimate the action value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Prediction \n",
    "\n",
    "In the previous concept, we saw that estimating the action value function with a Q table is an important intermediate step. We also refer to this as **prediction problem**.\n",
    "\n",
    "**Prediction Problem**: Given a policy, how might the agent estimate the value for the policy?\n",
    "\n",
    "We refer to Monte Carlo (MC) approaches to the predicition problem as **MC prediction method**.\n",
    "\n",
    "\n",
    "### Speical Case:\n",
    "Before we dig into the pseudocode, we note that there are two different  versions of MC prediction, depending on how we decide to treat special case where - **in a single episode - the same action is selected from the same state many times**\n",
    "\n",
    "\n",
    "* Monte Carlo Methods\n",
    "    1. First-visit MC\n",
    "    2. Every-visit MC\n",
    "    \n",
    "* **First-visit MC**: average the returns following **only first visit** to a state-action pair.\n",
    "* **Every-visit MC**: average the returns following __every visit__ to a state action pair.\n",
    "\n",
    "\n",
    "### Main Idea for First visit Mc prediction\n",
    "\n",
    "* $Q$-Q-table, with a row for each state and a column for each action. The entry corresponding to state s and action a is denoted $Q(s,a)$.\n",
    "* $N-$ table that keeps track of the number of first visits we have made to each state action pair.\n",
    "* $return_sum$ -table that keeps track of sum of the rewards obtained after visits to each state-action pair.\n",
    "\n",
    "In the algorithm, the number of episodes the agen collects is equal to $num_episodes$. After each episode, N and returns_sums are updated to store infromation contained in the episode. Then, after all of the episodes have been collected and the values in N aand returns_sum have been finalized, we quickly obtain the final estimate for Q.\n",
    "\n",
    "\n",
    "## First-visit or Ever-visit?\n",
    "\n",
    "Both the first-visit and every-visit method are guaranteed to converge to the true action-value function, as the number of visits to each state-action pair approaches infinity. (So, in other words, as long as the agent gets enough experience with each state-action pair, the value function estimate will be pretty close to the true value.) In the case of first-visit MC, convergence follows from the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), and the details are covered in section 5.1 of the [textbook](http://go.udacity.com/rl-textbook).\n",
    "\n",
    "\n",
    "If you are interested in learning more about the difference between first-visit and every-visit MC methods, you are encouraged to read Section 3 of this [paper](http://www-anw.cs.umass.edu/legacy/pubs/1995_96/singh_s_ML96.pdf).The results are summarized in section 3.6. The author show:\n",
    "\n",
    "* Ever vist MC is [biased](https://en.wikipedia.org/wiki/Bias_of_an_estimator), whereas the first-visit is unbiased (see Theorems 6 and 7).\n",
    "* Initially, every visit mc has lower mean square error, but more episodes are collected, first visit MC attain better MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLACKJACK RULES\n",
    "```python\n",
    "\"\"\"Simple blackjack environment\n",
    "\n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with an infinite deck (or with replacement).\n",
    "    The game starts with each (player and dealer) having one face up and one\n",
    "    face down card.\n",
    "\n",
    "    The player can request additional cards (hit=1) until they decide to stop\n",
    "    (stick=0) or exceed 21 (bust).\n",
    "\n",
    "    After the player sticks, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "\n",
    "    The observation of a 3-tuple of: the players current sum,\n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "\n",
    "    This environment corresponds to the version of the blackjack problem\n",
    "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto (1998).\n",
    "    http://incompleteideas.net/sutton/book/the-book.html\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Code for interacting with simulator/ environment\n",
    "\n",
    "```python\n",
    "for in range(num_episode):\n",
    "    ## starting point of new episodes\n",
    "    ## we want to randomly initialize the state at each  episode\n",
    "    state = env.reset()\n",
    "    \n",
    "    ##infinite loop till we get the terminal state:\n",
    "    while True:\n",
    "        print(state)\n",
    "        ##taking action from the policy which we want to optimize here in this example we are using equiproabable random actions\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        \n",
    "        ## the step method helps us traverse over states in the environment it has the information about the current state and the action taken by the agent it will give next proabable `state`,`reward`, and whether this state is terminal state or not and info about the state.\n",
    "        state,reward,done,info = env.step(action)\n",
    "        \n",
    "        ##checking done and terminanting the infinite loop.\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :) \\n') if reward > 0 else print(\"You list :( \\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes BlackJack\n",
    "```\n",
    "OBSERVATION SPACE (current_sum(player),dealer faceup card, usable ace (T/F))\n",
    "\n",
    "STICK : STOP [0]\n",
    "HIT : CARRY ON(draw more cards) [1] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Black Jack\n",
    "\n",
    "## Part 1: MC Prediction\n",
    "\n",
    "In this section, we will write our own implementation of Mc predicition (for estimating the action-value function).\n",
    "\n",
    "We will begin by investigating a policy where the player *almost* sticks if the sum of her cards exceeds 18. In particular, she `STICKS` with 80% probability if the sum is greater than 18; and, if the sum is 18 or below , she selects action `HIT` with 80% probability. The function samples an episode using this policy.\n",
    "\n",
    "The function accepts as the **input**:\n",
    "\n",
    "* `bj_env`: This is an instance of OpenAI Gym's environment.\n",
    "\n",
    "It returns as **output**:\n",
    "* `episode`: this is list of (State,action,reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        probs = [0.8,0.2] if state > 18 else [0.2,0.8]\n",
    "        action = np.random.choice(np.arange(2),p=probs)\n",
    "        next_state,reward,done,info = bj_env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = next_state\n",
    "        if done: break\n",
    "    return episode\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "returns_sum = defaultdict(lambda:np.zeros(2))\n",
    "print(returns_sum[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way defaultdicts are defined are as follows\n",
    "\n",
    "```\n",
    "return_sum[state][actions]\n",
    "```\n",
    "so here each state(key) has two corresponding actions so it will be initialize with `a list of size 2 with 0 as the value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "## First visit MC - action value!!\n",
    "```python\n",
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        temp_list = []\n",
    "        episode= generate_episode_from_limit_stochastic(env)\n",
    "        for i,exp_tuple in enumerate(episode):\n",
    "            state,action,_ = exp_tuple\n",
    "            N[state][action] += 1.0 \n",
    "            for j,exp_tuple2 in enumerate(episode[i:]):\n",
    "                _,_,reward = exp_tuple2\n",
    "                ## makes sure it's a single visit.\n",
    "                if (state,action) not in temp_list:\n",
    "                    returns_sum[state][action] += reward * (gamma ** j)\n",
    "            Q[state][action] = returns_sum[state][action]/ N[state][action]\n",
    "            temp_list.append((state,action))         \n",
    "        \n",
    "    return Q\n",
    "```\n",
    "\n",
    "<img src= images\\a7.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Policies\n",
    "\n",
    "\n",
    "## How to optimize the policy\n",
    "1. Collect episodes with $\\pi$(initial policy) & estimate the Q-table(action value).\n",
    "2. Use the Q-table to find a better policy $\\pi^{'}$ and set $\\pi \\gets \\pi^{'}$\n",
    "3. itterate over the above steps till the policy converge.\n",
    "4. But with above steps policy will not converge.\n",
    "\n",
    "\n",
    "When we take a Q-table and use the action that maximizes each row to come up with the policy, we say that we are constructing the policy that's **greedy with respect to the Q-table**, \n",
    "\n",
    "## New Steps\n",
    "1. Collect episodes with $\\pi$ and estimate the Q-table\n",
    "2. $\\pi^{'} \\gets greedy(Q)$ and $\\pi \\gets \\pi^{'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Policies\n",
    "\n",
    "* **The main difference between _EPSILON-GREEDY POLICIES_ and _GREEDY POLICIES_ is epsilon-greedy policies are stochastic policy and greedy policies are _determinstic_.**\n",
    "* As in earlier episode our agent is novice so it can get biased towards certain action which could be misleading so, stochasticity ensures we also try other actions too which initially were not giving high rewards but could be better actions.\n",
    "\n",
    "* Basically we tends to keep epsilon value **close to one** for initial episodes which results to keep our policy same as **equiprobable random policy** and then we reduces the value of **epsilon** gradually for later episodes.\n",
    "\n",
    "\n",
    "<img src = images\\a8.png>\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "We can think of the agent who follows an $\\epsilon$-greddy policy as always having a (potential unfair) coin at its disposal, with probability $\\epsilon$ of landing heads. After observing a state, the agen flips the coin.\n",
    "\n",
    "* If the coin lands tails (so with probability $1- \\epsilon$), the agent selects the greddy action.\n",
    "* If the coin lands head(so, with proabability $\\epsilon$), the agent selects an action uniformly at random from the set of available (non-greedy __AND__ greedy) actions.\n",
    "\n",
    "<img src = images\\a8.png>\n",
    "\n",
    "for each $s \\in S$ and $a \\in A(s)$. Note that $\\epsilon$ must always be a value between 0 and 1, inclusive (that is, $\\epsilon \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Few questions which will help us to understand the epsilon-greddy policy better\n",
    "\n",
    "1. Which of the values for epsilon yields an epsilon-greedy policy that is guaranteed to **always** selects the greedy action?\n",
    "    * epsilon = 0\n",
    "    \n",
    "    \n",
    "2. Which of the value for epsilon yields an epsilon-greedy policy that is guaranteed to **always** select a non-greedy action?\n",
    "    * This is a trick question! the *True answer* is that none of the values of epsilon satisfy this requirement\n",
    "    \n",
    "    \n",
    "3. Which of the values for epsilon yields an epsilon greedy policy that is equivalent to equiprobable random policy (where, from each state action is equally likely to be selected)?\n",
    "    * epsilon = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As long as epsilon > 0, the agent has nonzero probability if selecting any of the available actions.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Control\n",
    "\n",
    "Steps for optimal policy\n",
    "1. Using the policy $\\pi$ to construct the Q-table, and \n",
    "2. improving the policy by changing it to be $\\epsilon$-greedy with respect to the Q-table ($\\pi^{'} \\gets \\epsilon-greedy(Q)$ and $\\pi \\gets \\pi^{'}$\n",
    "\n",
    "\n",
    "We will eventually obtain the optimal policy $\\pi_*$\n",
    "\n",
    "Since this algorithm is a solution for **control problem** (defined below), we call it a **Monte Carlo control method**\n",
    "\n",
    "> **Control Problem:** Estimate the optimal policy\n",
    "\n",
    "It is common to refer to Step 1 as __policy evaluation__, since it is used to determine the action-value function of the policy. Likewise, since Step 2 is used to improve the policy, we also refer to it as a __policy improvement step__.\n",
    "\n",
    "\n",
    "### What we know till now\n",
    "So, using the new terminology, we can summarize that our **Monte Carlo method** alternates between **policy evaluation** and **policy improvement** steps to recover the optimal policy $\\pi_*$\n",
    "\n",
    "\n",
    "## The Road Ahead \n",
    "***\n",
    "\n",
    "We now have a working algorithm for Monte Carlo control! So what's to come?\n",
    "\n",
    "* In the next concept (**Exploration vs. Exploitation)**, we will learn more about how to set the value of $\\epsilon$ when constructing $\\epsilon - greedy$ policies improvement steps.\n",
    "* Then, we will learn about two improvements that we can make to policy evaluation step in our control algorithm.\n",
    "    * In the **Incremental Mean** concept, we will learn how to update the policy after every episode (instead of waiting to update the policy until after the values of the Q-table have fully converged for many episodes).\n",
    "    * In the **Constant-alpha** concept, we will learn how to train the agent to leverage its most recent experience more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration vs. Exploitation\n",
    "\n",
    "\n",
    "### Solving Environment in OpenAI Gym\n",
    "***\n",
    "\n",
    "In many cases, we would like our reinfocement learning (RL) agens to learn as quickly as possible. This can be seen in many OpenAI Gym environments.\n",
    "\n",
    "For instance, the FrozenLake-v0 environment is considered solved once the agent attains an average reward of 0.78 over 100 consecutive trials.\n",
    "\n",
    "### Exploration vs. Exploitation Dilemma\n",
    "***\n",
    "\n",
    "Recall the the evironment's dynamics are initially unkown to the agent. Towards maximizing return, the agent must learn about the environment through interaction.\n",
    "\n",
    "At every time step, when the agent selects an action, it bases its decision on past experience with the environment. And, towards minimizing the number of episodes needed to solve environments in OpenAI Gym, our first instinct could be to devise a strategy where the agent always selects the action that it believes (based on its past experience) will maximize return. With this in mind, the agent could follow the policy that is greedy with respect to the action-value function estimate. We examined this approach in a previous video and saw that it can easily lead to convergence to a **sub-optimal policy**.\n",
    "\n",
    "\n",
    "To see why this is the case, note that in early episodes, the agent's knowledge is quite limited (and potentially flawed). So, it is highly likely that actions estimated to be non-greedy by the agent are in fact better than the estimated greedy action.\n",
    "\n",
    "With this in mind, a successful RL agent cannot act greedily at every time step (that ism it cannot always **exploit** its knowledge); instead, in order to discover the optimal policy, it has to continue to refine the estimated return for all states-action pairs (in other words, it has to continue to **explore** the range of possibilites by visiting every state action pair). That said, the agent should always act somewhat greedily, towards its goal of maximizing return as quickly as possible. This motivated the idea of an $\\epsilon-$ greedy policy.\n",
    "\n",
    "We refer to the need of balance these two competing requirement as the **Exploration-Exploitation Dilemma**. One potential solution to this dilemma is implemented by gradually modifying the value of $\\epsilon$ when constructing $\\epsilon$ - greedy policies.\n",
    "\n",
    "\n",
    "### Setting the Vale of Epsilon, In Theory\n",
    "***\n",
    "\n",
    "It makes sense for the agent to begin its interaction with the environment by favoring **exploration** over **exploitation**. After all, when the agent knows relatively little about the environment's dynamics, it should distrust its limited knowledge and **explore**, or try out various strategies for maximizing return. With this in mind, the best starting policy is the equiprobable random policy, as it is equally likely to explore all possible actions from each state. You discovered in the previous quiz that setting $\\epsilon =1$ yields an $\\epsilon$-greedy policy that is equivalent to the equiprobable random policy.\n",
    "\n",
    "\n",
    "At later time steps, it makes sense to favor **exploitation** over **exploration**, where the policy gradually becomes more greedy with respect to the action-value function estimate. After all, the more the agent interacts with the environment, the more it can trust its estimated action-value function. You discovered in the previous quiz that setting $\\epsilon =0$ yields the greedy policy (or, the policy that most favors exploitation over exploration).\n",
    "\n",
    "Thankfully, this strategy (of initially favoring exploration over exploitation, and then gradually preferring exploitation over exploration) can be demonstrated to be optimal.\n",
    "\n",
    "\n",
    "## Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "***\n",
    "\n",
    "In order to guarantee that MC control converges to the optimal policy $\\pi_*$, we need to ensure that two conditions are met. We refer to these conditions as **Greedy in the Limit with Infinite Exploration (GLIE)**\n",
    "\n",
    "* every state-pair $s,a$ (for all $s \\in S$ and $a \\in A(s)$ is visited infinitely many times, and \n",
    "* the policy converges to a policy that is greedy with respect to the action-value function estimate $Q$.\n",
    "\n",
    "then MC control is guaranteed to converge to the optimal policy (in the limit as the algorithm is run for infinitely many episodes). These conditions ensure that:\n",
    "* the agent continue to explore for all time steps, and \n",
    "* the agent gradually exploits more (and explores less).\n",
    "\n",
    "One way to satisfy these conditions is to modify the value of $\\epsilon$ when specifying an $\\epsilon$-greedy policy. In particular,  let $\\epsilon _ {i}$ corresponds to the i-th time step. then both of the condition is met if:\n",
    "\n",
    "* $\\epsilon _ {i} > 0$ for all time steps i, and\n",
    "* $\\epsilon _ {i}$ decays to zero in the limit as the time step $i$ approaches infinity (that is, $lim_{i\\to \\inf} = 0$)\n",
    "\n",
    "For example to ensure convergence to the optimal policy we could set $\\epsilon_{i} = \\frac{1}{i}$ for all $i$, and check $lim_{i\\to \\inf} = 0$\n",
    "\n",
    "\n",
    "\n",
    "## Setting the value of epsilon in Practice\n",
    "*** \n",
    "\n",
    "As we read in above section, in order to guarantee convergence, we must let $\\epsilon_{i}$ decay in accordance with GLIE conditions. But sometimes \"guaranteed convergence\" isn't good enough for practice, since this really dosen't tell us how long we have to wait! It is possible that we could need trillions episodes to recover the optimal policy, for instance, and the \"guaranteed covergence\" would still be accurate!\n",
    "\n",
    "> Even though covergence is **not** guaranteed by the mathematicsm we can often get better results by either:\n",
    "* using fixed $\\epsilon$\n",
    "* letting $\\epsilon_{i}$ decay to small positive number like 0.1\n",
    "\n",
    "\n",
    "This is because one has to be very careful with setting the decay rate for $\\epsilon$; letting it get too small too fast can be disastrous. If you get late in training and $\\epsilon$ is really small, you pretty much want the agent to have already converged to the optimal policy, as it will take way too long otherwise for it to test out new actions!\n",
    "\n",
    "As a famous example in practice, you can read more about how the value of \\epsilonϵ was set in the famous DQN algorithm by reading the Methods section of the [research paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf):\n",
    "\n",
    "> The behavior policy during training was epsilon-greedy with epsilon annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Mean\n",
    "\n",
    "In our current algorithm for Monte Carlo control, we collect a large number of episodes to build the Q-table (as an estimate for the action value function corresponding to agent's policy). Then, after the values in the Q-table have converged, we use the table to come up with an improved policy.\n",
    "\n",
    "Maybe it would be more efficient to update the Q-table **After every episode**. Then, the updated Q-table could be used to improve the policy. That new policy could then be used to generate the next episode, and so on.\n",
    "\n",
    "## Running Mean Formula\n",
    "$Q \\gets Q + \\frac{1}{N}(G-Q)$\n",
    "\n",
    "After each episode, we can calculate a **new action value estimate**\n",
    "from the **old action value estimate** the most recently sampled return, and the total number of visits to the state-action pair.\n",
    "\n",
    "There are two relevant tables:\n",
    "* $Q$ - Q-table, with a row for each state and a column for each action. The entry corresponding to state $s$ and action $a$ is dentoed $Q(s,a)$.\n",
    "* $N$ - table that keeps track of the large number of first visits we have made to each state action pair.\n",
    "\n",
    "The number of episodes the agent collects is equal to $num_episodes$\n",
    "\n",
    "The algorithms proceeds by looping iver the following steps:\n",
    "* the policy $\\pi$ is improved to be $\\epsilon$-greedy with respect to $Q$, and the agent uses $\\pi$ to collect an episode.\n",
    "* $N$ is updated to count the total number of first visits to each state action pair.\n",
    "* The estimate in Q are updated to take into account the most recent information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant - alpha\n",
    "\n",
    "#### Equation\n",
    "> $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\frac{1}{N(S_t,A_t)}(G_t - Q(S_t,A_t))$\n",
    "\n",
    "We consider this term as error\n",
    "\n",
    "__Error__ : $(G_t - Q(S_t,A_t))$\n",
    "\n",
    "So this tell us whether we have to add value to our previous action value or we have to substract.\n",
    "\n",
    "**So if we divide the error with N then in later steps when N will be quite large then update will be quite small which is not *desired*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the value of Alpha\n",
    "\n",
    "Recall the update equation that we use to amend the values in the Q-table:\n",
    "\n",
    "> $Q(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha(G_t - Q(S_t,A_t))$\n",
    "\n",
    "To examine how to set the value of the $\\alpha$ in more detail, we slightly rewrite the equation as follows:\n",
    "> $Q(S_t,A_t) \\gets (1 - \\alpha)Q(S_t,A_t) + \\alpha G_t$\n",
    "\n",
    "* ALPHA SHOULD BE CLOSE TO ZERO -- suggested\n",
    "\n",
    "Here are some guiding principles that will help us to set the value of $\\alpha$ when implementing constant - $\\alpha$ MC control:\n",
    "* We should always set the value for $\\alpha$ to a number greater than 0 and less than or equal to one.\n",
    "    * If $\\alpha = 0 $ then action value function estimate is never updated by the agent.\n",
    "    * If $\\alpha = 1$, the the final value estimate for each state-action pair is always eqaul to last return that was experienced by the agent (after visiting the pair).\n",
    "    \n",
    "* Smaller values for $\\alpha$ encourages the agent to consider a longer history of returns when calculating the action value function estimate. Increasing the value of the $\\alpha$ ensures that the agent focuses more on the most recent sampled returns.\n",
    "\n",
    "> Important Note: When implementing constant-$\\alpha$ MC control, you must be careful to not set the value of $\\alpha$ too close to 1. This is because very large values can keep the algorithm from converging to the optimal policy $\\pi_*$ However, you must also be careful to not set the value of $\\alpha$ too low, as this can result in an agent who learns too slowly. The best value of $\\alpha$ for your implementation will greatly depend on your environment and is best gauged through trial-and-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
