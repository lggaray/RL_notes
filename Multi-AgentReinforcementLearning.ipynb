{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Multi-AgentRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Multi-agent Systems\n",
    "* Ultimate goal of AI is to solve intelligence.\n",
    "* We live in a multiagent world, we do not become intelligent in isolation. \n",
    "* We learn from other and our own experiences, and so on.\n",
    "* Our intelligence is therefore a result of our iteractions with multiple agents over our lifetime.\n",
    "* If we want to build intelligent agents that are used in real world, they have to interact with humans, and also with other agents.\n",
    "* This lead to multi agent scenario.\n",
    "* The multi-agent case is a very complex kind of environment because all the agent are learning simultaneously and also iteracting with one another.\n",
    "### Summary\n",
    "* We live in a multi agent world\n",
    "* Intelligent agents have to interact with human \n",
    "* Agents need to work in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Multi-Agent Systems\n",
    "* Some potential real-life application of multi-agent systems.\n",
    "1. A group of drones or robots whose aim is to pick up a package and drop it to the destination is a multi-agent systems.\n",
    "2. In the stock market, each person who is trading can be considered as an agent and the profit maximization can be modeled as a multi-agent problem.\n",
    "3. Interactive robots or humanoids that can iteracts with humans and get some task done are nothing but multi-agent system if we consider humans to be agent.\n",
    "4. Windmills in a wind farm can be thought of as multiple agents.\n",
    "    * It would be cool if the agents, that is, the wind turbines figured out the optimal directions to face by themselves, and obtain maximum energy from the wind farm.\n",
    "    * The aim here is to collaboratively maximize the profit obtained from the wind farm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Multi-Agent Systems\n",
    "* The agents can share their experiences with one another making each other smarter, just as we learned from our teachers and friends.\n",
    "* However, when agents want to share, they have to communicate, which leads to a cost of communication, like extra hardware and software capabilites.\n",
    "* A multi-agent system is robust.\n",
    "* Agent can be replaced with a copy when they fail. Other agents in the system can take over the tasks of the failed agent, but the substituting agent now has to  do some extra work.\n",
    "* Scalability comes by virtue of design, as most multi-agent system allow insertion of new agent easily.\n",
    "* But if more agent are added to the system, the system becomes more complex than before.\n",
    "* So it depends on the assumptions made by the algorithm and the software-hardware capabilities of the agents, whether or not these advantages will be exploited.\n",
    "* So from here onwards, we will learn about multi-agent RL, also known as $$MARL$$\n",
    "* When multi-agent system used reinforcement learning techinques to train the agent and make them learn their behaviours, we call the process **multi-agent reinforcement learning**.\n",
    "* Next we learn about the __framework__ for $MARL$ just like Markov decision processes are __MDPs__ for __single-agent RL__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Games\n",
    "* Consider an example of single agent reinforcement learning.\n",
    "* We have a drone with the task of grabbing a package. The possible action actions are going right, left, up, down, and grasping.\n",
    "* The reward is +50 for grasping the package, and minus one (-1) otherwise.\n",
    "* Now the difference in multi-agent RL, is that we have more than one agent. So say we have a second drone. Now both the agent collaboratively trying to grasp the package.\n",
    "* They're both observing the packets from their respective positions.\n",
    "* They both have their own policies that returned an action for their observations.\n",
    "* Both have their own set of actions. **The main thing about multi-agent RL, is that there is also a __joint set actions__.**\n",
    "* Both the left drone and right drone must begin action.\n",
    "* For example, the pair DL is bended left drone moves down, and right drone moves to left.\n",
    "* This example illustrates the Markov game framework, which we are now ready to discuss in more detail.\n",
    "\n",
    "\n",
    "* A markov game, is a tuple written as this:\n",
    "$$(n,S,A_1,...,A_n,O_1,...,O_n,R_1,...,R_n,\\pi_1,...,\\pi_n,T)$$\n",
    "    - $n$: number of agents\n",
    "    - $S$: set of environment states\n",
    "    - $A$: $A_1 \\times A_2,... \\times A_n$($A_i$ is set of actions of agent i) $A$ is joint action space.\n",
    "    - $O_i$:$O_i$ is set of observation of agent i\n",
    "    - $R$: $S \\times A_i \\to R$($R_i$ is the reward function of agent i) which returns a real value for acting in action in a particular state,\n",
    "    - $\\pi_i$:$O_i \\to A_i$ ($\\pi_i$ is the policy of each agent i) given the observation returns the probability distribution over actions $A_i$.\n",
    "    - $T$: $S \\times A \\to S$($T$ is the state transition function, given the current state and the joint action, it provides a probability distribution over the set of possible next_states.)\n",
    "    \n",
    "    \n",
    "* Note, that even here the state transition are Markovian, just like in an MDP. Recall **Markovian** means that the **next state depends only on the present state and the action taken in this state.**\n",
    "* However, this transition function now depends on the **joint action**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to MARL\n",
    "* So can we think about adapting the single-agent RL techniques we've learned about so far to the multi-agent case?\n",
    "* Two extreme approaches comes to mind.\n",
    "* The simplest approach should be to train all the agents **independently** without considering the **existence** of other agents. \n",
    "* In this approach, any agent **considers all the others to be a part of the environment** and learns its own policy.\n",
    "* Since all are learning simultaneously, the environment as seen from the prospective of a single agent, **change dynamically**.\n",
    "* This condition is called **NON-STATIONARITY** of the environment.\n",
    "* In most single agent algorithms, it is assumed that the environment is **Stationary**, which leads to certain **convergence** guarantees.\n",
    "* Hence, under **non-stationarity** conditions, these **guarantees of convergence no longer holds**.\n",
    "#### Second Approach\n",
    "* The second approach is the **matter** agent approach.\n",
    "* The matter agent approach takes into account the **exsistence of multiple agents.**\n",
    "* Here, a single policy is knowed for all the agents. It takes as input the present state of environment and returns action of each agent in the form of a single joint **Action vector**.\n",
    "$$Policy: S\\to A_1 \\times A_2 ... \\times A_n$$\n",
    "* Typically, a **single reward function, given the environment state and the action vector returns a GLOBAL REWARD**\n",
    "$$R: S \\times A \\to Real\\ Number$$\n",
    "* The joint action space as we have discussed before, would increase **exponentially** with the number of the agents.\n",
    "* If environment is **partially observable** or the agent can only see locally, each agent will have a different **observation** of the environment state, hence it will e difficult to **disambiguate** the state of the environment from different local observations.\n",
    "* So this approach work well only when each agent knows **Everything** about the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooperation, Competition, Mixed Environments\n",
    "#### Example Condition\n",
    "### CASE 1\n",
    "* Let's pretend that you and your sister are playing a game of pong.\n",
    "* We are give one bag of 100 coins frow which we plan buying a video game console.\n",
    "* For each time either of us misses the ball, we lose one coin from the bank to our parents.\n",
    "* Hence, we both will try to keep the ball in the game to have as many coins as possible at the end. -- __cooperation__\n",
    "* This is an example of cooperative environment where the agents are **Concerned** about to accomplishing a group task and cooperate to do so.\n",
    "### CASE 2\n",
    "* Consider that now we both have separate banks.\n",
    "* Whosoever misses the ball, gives a coin from their bank to the other.\n",
    "* So now instead of cooperating, we're competing with each another.\n",
    "* One sibling's gain is the other's loss.\n",
    "* This is an example competitive environment where the agents are just concerned about maximizing their own rewards.\n",
    "\n",
    "\n",
    "* Notice in cooperative setting both of us loses a coin while in the competitive setting, while in competitive setting one loses a coin when other gains a coin.\n",
    "* So, the way reward is defined makes the agent behaviour competitive or apparently collaborative.\n",
    "* In many environments, the agents have to show a mixture of both(cooperative and competitive behaviours which leads to mixed cooperative competitive environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Topics\n",
    "* The field of mult-agent RL is abuzz with cutting edge research.\n",
    "* Recently, Open AI announced that its team of five neural networks, OpenAI 5 has learned to defeat amature DOTA 2 players.\n",
    "* OpenAI 5 has been trained using a scaled-up version of **BPO**\n",
    "* Coordination between agents is controlled using hyperparameter called `team_spirit`.\n",
    "* It range from zero to one, where zero means agent only care about the **individual reward** function while one means that they completely care about the team's reward function. \n",
    "\n",
    "There are many iteresting papers out there on MARL. For the purposes of this lesson we will stick to one particular paper called [Multi Agent Actor Critic for Mixed Cooperative Competitive environment](https://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf) by OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Description, Part 1\n",
    "* The paper we have chosen implements a multi-agent version of DDPG.\n",
    "* DDPG, as we might remember, is an off-policy actor-critic algorithm that uses the concept of traget networks.\n",
    "* The input of the actor network is the current state while output is real value or a vector representing an action chosen from a continuous action space.\n",
    "* OpenAI has created a mulit-agent environment called multi-agent particle.\n",
    "* It consists of particle that are agents and some landmarks.\n",
    "* A lot of itresting experimental scenarios have been laid out in this environment.\n",
    "* We will be working one of many scenarios called physical deception.\n",
    "* Here, any agents cooperate to reach the target landmark out of n landmarks.\n",
    "* There is an adversary which also trying to reach the target landmark, but it doesn't know which out of n landmarks is the target landmark.\n",
    "<img src = \"images/a23.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Description, Part 2\n",
    "* The normal agents are rewarded based on the least distance of any of the agents to the landmark, and penalized based on the distance between the adversary and the target network.\n",
    "* Under this reward structure, the agent cooperate to spread out across all the landmarks, so as to deceive the adversary. \n",
    "\n",
    "* The framework of centralized training with decentralized execution has been adopted in this paper.\n",
    "* This implies that some extra information is used to ease training, but that information is not used during the testing time.\n",
    "* This frame can be naturally implemented using actor-critic algorithm.\n",
    "#### Important\n",
    "* During training, the __critic__ for each agent uses **extra information** like __state's observed and actions taken by all the other agents__.\n",
    "* As for the actor we'll notice that there is one for each agent.\n",
    "* Each actor has access to only its agent's observation and actions. \n",
    "* During execution time, only the actors are present and hence, own observation and actions are used.\n",
    "<img src = \"images/a24.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "* We began by introducing ourselves to multi-agent system present in our surroundings.\n",
    "* We reasoned why multi-agent system are an important puzzle to solve AI, and decided to pursue this complex topic. \n",
    "* We also studied the Markov game framework, which is generalization of MDPs to the multi-agent case.\n",
    "* We talked about using single-agent RL algorithms, as they are in multi-agent case.\n",
    "* This either leads to **non-stationarity**, or a large joint action space. \n",
    "* We saw the intresting kinds of environments presents in the multi-agent case namely: cooperative, competitive and mixed.\n",
    "* Towards the end, we implemented multi-agent DDPG algorithm which is **centralized training**, and **decentralized execution** algorithm that can be used in any of the above environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project -- Physical Deception\n",
    "For this Lab, we will train an agent to solve the **Physical Deception** problem.\n",
    "\n",
    "### Goal of the environment\n",
    "Blue dots are **good agent**, and the RED DOTS are **adversary**. All the agents' goals are to go near the green target. The blue agents know which one is green, but the Red agent is color blind and does not know which is green/black! The optimal solution os for the red agent to chase one of the blue agent, and for the blue agent, and for the blue agents to split up and go towards each of the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running within the workspace ( Recommended Option)\n",
    "No explicit setup commands need to run by you, we have taken care of all the installations in this lab, enjoy exploration.\n",
    "./run_training.sh Let's you run the program based on the parameters provided in the main program.\n",
    "./run_tensorboard.sh will give you an URL to view the dashboard where you would have visualizations to see how your agents are performing. Use this as a guide to know how the changes you made are affecting the program.\n",
    "Folder named Model_dir would store the episode-XXX.gif files which show the visualization on how your agent is performing.\n",
    "\n",
    "* `torch.norm()` computation $||x||_{p} = \\sqrt[p]{x_{1}^{p} + x_{2}^{p} + \\ldots + x_{N}^{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADDPG LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## networkforall.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1./np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_in_dim, hidden_out_dim,\n",
    "                 output_dim, actor=False):\n",
    "        super(Network,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_in_dim)\n",
    "        self.fc2 = nn.Linear(hidden_in_dim,hidden_out_dim)\n",
    "        self.fc3 = nn.Linear(hidden_out_dim,output_dim)\n",
    "        self.nonlin = f.relu ## leaky_relu\n",
    "        self.actor = actor\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-1e-3, 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.actor:\n",
    "            h1 = self.nonlin(self.fc1(x))\n",
    "            h2 = self.nonlin(self.fc2(h1))\n",
    "            h3 = self.fc3(h2)\n",
    "            norm = torch.norm(h3)\n",
    "            # h3 is a 2D vector (a force that is applied to the agent)\n",
    "            # we bound the norm of the vector to be between 0 and 10\n",
    "            return 10.0*(f.tanh(norm))*h3/norm if norm > 0 else 10*h3\n",
    "        \n",
    "        else:\n",
    "            ## critic network simply outputs a number \n",
    "            h1 = self.nonlin(self.fc1(x))\n",
    "            h2 = self.nonlin(self.fc2(x))\n",
    "            h3 = (self.fc3(h2))\n",
    "            return h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddpg.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual network settings for each actor+crtic pair\n",
    "\n",
    "## see networkforall for details\n",
    "from networkforall import Network\n",
    "from utilities import hard_update, gumbel_softmax, onehot_from_logits\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ad OU noise for exploration\n",
    "from OUNoise import OUNoise\n",
    "device = \"cpu\"\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self,in_actor,hidden_in_actor, hidden_out_actor, out_actor,\n",
    "                in_critic, hidden_in_critic, hidden_out_critic, lr_actor=1.0-2,\n",
    "                lr_critic=1.0e-2):\n",
    "        super(DDPGAgent, self)__init__()\n",
    "        self.actor = Network(in_actor, hidden_in_actor, hidden_out_actor,\n",
    "                            out_actor,actor=True).to(device)\n",
    "        self.critic = Network(in_critic, hidden_in_critic, hidden_out_critic,\n",
    "                            1).to(device)\n",
    "        self.target_actor = Network(in_actor, hidden_in_critic, hidden_out_actor,\n",
    "                                   out_actor, actor=True).to(device)\n",
    "        self.target_critic = Network(in_critic, hidden_in_critic, hidden_out_critic,\n",
    "                                    1).to(device)\n",
    "        self.noise = OUNoise(out_actor,scale=1.0)\n",
    "        \n",
    "        ##initializing target same as original network\n",
    "        self.hard_update(self.target_actor,self.actor)\n",
    "        self.hard_update(self.target_critic, self.critic)\n",
    "        \n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr= lr_critic, weight_decay=1.0e-5)\n",
    "        \n",
    "    def act(self, obs, noise=0.0):\n",
    "        obs = obs.to(device)\n",
    "        action = self.actor(obs)+ noise*self.noise.noise()\n",
    "        return action\n",
    "    \n",
    "    def target_act(self, obs, noise=0.0):\n",
    "        obs = obs.to(device)\n",
    "        action = self.target_actor(obs)+noise*self.noise.noise()\n",
    "        return action\n",
    "    \n",
    "    def hard_update(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(),source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def soft_update(self, target, source, tau):\n",
    "        for target_param,param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data*(1-tau) + tau*param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maddpg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main code that contains the neural network setup\n",
    "# policy + critic updates\n",
    "\n",
    "from ddpg import DDPGAgent\n",
    "import torch\n",
    "from utilities import soft_update, transpose_to_tensor, transpose_list\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, discount_factor=0.95, tau=0.02):\n",
    "        super(MADDPG, self).__init__()\n",
    "        \n",
    "        # critic input = obs_full + actions = 14(state_space)+2(action_agent1)+ 2 (action_agent2)+ 2 (action_agent3) = 20\n",
    "        ## Total three agent one adversary and 2 good agent!!\n",
    "        self.maddpg_agent = [DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                            DDPGAgent(14, 16, 8, 2, 20, 32, 16),\n",
    "                            DDPGAgent(14, 16, 8, 2, 20, 32, 16)]\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.iter = 0\n",
    "        \n",
    "    def get_actors(self):\n",
    "        \"\"\"get actors of all the agents in the MADDPG object\"\"\"\n",
    "        actors = [ddpg_agent.actor for ddpg_agent in self.maddpg_agent]\n",
    "        return actors\n",
    "    \n",
    "    def get_target_actors(self):\n",
    "        \"\"\"get target_actors of all the agents in the MADDPG object\"\"\"\n",
    "        target_actors = [ddpg_agent.target_actors for ddpg_agent in self.maddpg_agent]\n",
    "        return target_actors\n",
    "    \n",
    "    def act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        actions = [agent.act(obs, noise) for agent, obs in zip(self.maddpg_agent, obs_all_agents)]\n",
    "        return actions\n",
    "    \n",
    "    def target_act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"Get target network actions from all the agent in the MADDPG object\"\"\"\n",
    "        \n",
    "        target_actions = [ddpg_agent.target_act(obs, noise) for ddpg_agent,obs in zip(self.maddpg_agent, obs_all_agents)]\n",
    "        return target_actions\n",
    "    \n",
    "    def update(self, samples, agent_number, logger):\n",
    "        \"\"\"update the critics and actors of all the agents\"\"\"\n",
    "        \n",
    "        #need to transpose each element of the samples\n",
    "        #to flip obs ::  dim -> [parallel_agent][agent_number] to \n",
    "        # obs :: dim -> [agent_number][parallel_agent]\n",
    "        obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensor, samples)\n",
    "        \n",
    "        obs_full = torch.stack(obs_full)\n",
    "        next_obs_full = torch.stack(next_obs_full)\n",
    "        \n",
    "        agent = self.maddpg_agent[agent_number]\n",
    "        \n",
    "        ## ============================== ##\n",
    "        #       Critic Training            #\n",
    "        ## ============================== ##\n",
    "        \n",
    "        agent.critic_optimizer.zero_grad()\n",
    "        \n",
    "        ## CRITIC LOSS = batch mean of (y - Q(s,a) from target network)^2\n",
    "        ## y = reward from this timestep + discount* Q(st+1, at+1) from target network\n",
    "        ## at+1 from actor_target.\n",
    "        target_actions = self.target_act(next_obs) #size (3,2) #from three agent\n",
    "        target_actions = torch.cat(target_actions, dim=1) #size (1,6)\n",
    "        \n",
    "        target_critic_input = torch.cat((next_obs_full.t(),target_actions),dim=1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_next = agent.target_critic(target_critic_input)\n",
    "        y = reward[agent_number].view(-1,1) + self.discount_factor * q_next * (1 - done[agent_number].view(-1,1))\n",
    "        \n",
    "        action = torch.cat(action, dim=1)\n",
    "        critic_input = torch.cat((obs_full.t(),action), dim=1).to(device)\n",
    "        q = agent.critic(critic_input)\n",
    "        \n",
    "        huber_loss = torch.nn.SmoothL1Loss()\n",
    "        critic_loss = huber_loss(q,y.detach())\n",
    "        critic_loss.backward()\n",
    "        \n",
    "        agent.critic_optimizer.step()\n",
    "        ## ============================== ##\n",
    "        #         Agent Training           #\n",
    "        ## ============================== ##\n",
    "        \n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        # make input to agent\n",
    "        # detach the other agents to save computing derivation\n",
    "        # >>>>>>> read about .detach() method <<<<<<<<\n",
    "        # saves some time for computing derivative\n",
    "        \n",
    "        q_input = [self.maddpg_agent[i].actor(ob) if i == agent_number \\ \n",
    "                  else  self.maddpg_agent[i].actor(ob).detach() for i, ob in enumerate(obs)]\n",
    "        q_input = torch.cat(q_input, dim=1)\n",
    "        # combine all the actions and observations for input to critc\n",
    "        # many of the obs are redundant, and obs[1] contains all useful infromation already\n",
    "        \n",
    "        q_input2 = torch.cat((obs_full.t(), q_input), dim =1)\n",
    "        \n",
    "        # get the policy gradient\n",
    "        actor_loss = -agent.critic(q_input2).mean()\n",
    "        actor_loss.backward()\n",
    "        agent.actor_optimizer.step()\n",
    "        \n",
    "        al = actor_loss.cpu().detach().item()\n",
    "        cl = critic_loss.cpu().detach().item()\n",
    "        logger.add_scalars('agent%i/losses' % agent_number,\n",
    "                   {'critic loss': cl,\n",
    "                    'actor_loss': al},\n",
    "                   self.iter)\n",
    "    def update_targets(self):\n",
    "        \"\"\"soft update targets\"\"\"\n",
    "        self.iter +=1\n",
    "        for ddpg_agent in self.maddpg_agent:\n",
    "            ddpg_agent.soft_update(ddpg_agent.target_actor, ddpg_agent.actor, self.tau)\n",
    "            ddpg_agent.soft_update(ddpg_agent.target_critic, ddpg_agent.critic, self.tau)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4740,  0.2601],\n",
       "        [ 0.9360, -0.2556],\n",
       "        [ 0.6471, -1.2109]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7660)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
